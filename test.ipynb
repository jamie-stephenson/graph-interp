{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. **Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Graph, get_dataloader\n",
    "from utils.interp import *\n",
    "from utils.dataset.build import generate_planar, generate_misclass \n",
    "from models import get_model\n",
    "\n",
    "from transformer_lens import FactoredMatrix\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import networkx as nx\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import einops\n",
    "import wandb\n",
    "from jaxtyping import Float\n",
    "import numpy as np\n",
    "import rich\n",
    "from rich.table import Table\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'trained_models/transformer_2024-09-05_13-28-07'\n",
    "\n",
    "wandb.init(\n",
    "    config=path+'.yaml',\n",
    "    mode='disabled'\n",
    ")\n",
    "\n",
    "args = wandb.config\n",
    "\n",
    "model = get_model(args)\n",
    "model.eval()\n",
    "device = model.cfg.device\n",
    "state_dict = torch.load(path+'.pt',map_location=device)\n",
    "model.load_state_dict(state_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test model on val set and random graphs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = get_dataloader('adj','data/n10_15-21_tn_vn/val.npz',1)\n",
    "n_planar = n_correct = 0\n",
    "for x,y in loader:\n",
    "    # plot_graph(adj_to_graph(x.squeeze()))\n",
    "    logits = model(x)\n",
    "    correct = logits.argmax()==y\n",
    "    n_planar += y.item()*correct\n",
    "    n_correct += correct\n",
    "\n",
    "print(f\"Val Set Accuracy: {(100*n_correct/len(loader)).item():.2f}%\")\n",
    "print(f\"{(100*n_planar/n_correct).item():.2f}% of correctly classified graphs were planar.\")\n",
    "print()\n",
    "\n",
    "for m in range(22,30):\n",
    "    n_planar = n_correct = 0\n",
    "    n_samples = 1000\n",
    "    for _ in range(n_samples):\n",
    "        g = Graph(nx.gnm_random_graph(args.n_vertices,m))\n",
    "        y = nx.is_planar(g.G)\n",
    "        logits = model(g.A)\n",
    "        correct = logits.argmax()==y\n",
    "        n_planar += y*correct\n",
    "        n_correct += correct\n",
    "\n",
    "        # print(f\"y: {y}\")\n",
    "        # print(f\"logits.argmax(): {logits.argmax()}\")\n",
    "        # print(f\"correct: {correct}\")\n",
    "        # plt.figure(figsize=(8, 6))\n",
    "        # nx.draw(g, with_labels=True, node_color='lightblue', edge_color='gray', node_size=500, font_size=10)\n",
    "        # plt.title(\"Random Graph\")\n",
    "        # plt.show()\n",
    "\n",
    "    print(f\"Accuracy on {n_samples} random graphs with {m} edges: {(100*n_correct/n_samples).item():.2f}%\")\n",
    "    print(f\"{(100*n_planar/n_correct).item():.2f}% of correctly classified graphs were planar.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cache activations from example graphs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 17\n",
    "\n",
    "random_graph = Graph(nx.gnm_random_graph(model.cfg.n_vertices,m))\n",
    "petersen_graph = Graph(nx.petersen_graph())\n",
    "cycle_graph = Graph(nx.cycle_graph(model.cfg.n_vertices))\n",
    "complete_graph = Graph(nx.complete_graph(model.cfg.n_vertices))\n",
    "empty_graph = Graph(nx.empty_graph(model.cfg.n_vertices))\n",
    "star_graph = Graph(nx.star_graph(model.cfg.n_vertices-1))\n",
    "\n",
    "loader = get_dataloader('adj','data/n10_15-21_tn_vn/val.npz',1024)\n",
    "batch,batch_labels = next(iter(loader))\n",
    "\n",
    "model.set_use_attn_result(True)\n",
    "_, random_cache = model.run_with_cache(random_graph.A)\n",
    "_, petersen_cache = model.run_with_cache(petersen_graph.A)\n",
    "_, cycle_cache = model.run_with_cache(cycle_graph.A)\n",
    "_, complete_cache = model.run_with_cache(complete_graph.A)\n",
    "_, empty_cache = model.run_with_cache(empty_graph.A)\n",
    "_, star_cache = model.run_with_cache(star_graph.A)\n",
    "_, batch_cache = model.run_with_cache(batch)\n",
    "model.set_use_attn_result(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_graph.plot_attention(random_cache['pattern',1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_planar = 0\n",
    "for g in batch:\n",
    "    graph = Graph(g)\n",
    "    n_planar += nx.is_planar(graph.G)\n",
    "print(f\"{100*n_planar/len(batch):.2f}% of the batch are planar graphs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. **Identifying a Circuit**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 **Plot attention patterns**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by looking at the attention patterns for specific inputs to the model. In the case of our model, attention patterns $A^h$ are 10 by 10 matrices that represent how each attention head $h$ moves information between the vertex positions of the input graph. For instance, if $A_{ij}^{1.3}$ is large, that means that head 1.3 moves lots of information from vertex position $j$ to vertex position $i$. We say that vertex position $i$ *attends strongly* to vertex postion $j$.\n",
    "\n",
    "I took the usual [CircuitsVis](https://github.com/TransformerLensOrg/CircuitsVis) way of visualising language model attention patterns (in which you visualise which tokens an input sentence attend to each other on the tokens themselves) and applied it to graph inputs. This means you can select an attention pattern and hover over a vertex to see which other vertices it attends to. \n",
    "\n",
    "It is important to remember that transformers are designed so that the attention patterns in each attention head *depend on the input*. Indeed, if we rename all the vertices of a graph but keep all the edges the same, we still want the vertices to attend to each other in the same way and so the model must adjust its attention patterns accordingly. It is therefore important to study the attention patterns for a whole range of input graphs. This way you can get a much better idea of what each head could be doing.\n",
    "\n",
    "Have a go yourself at looking at the attention patterns for different input graphs. The example below is for the attention patterns in layer 0 for a randomly generated graph, but you can swap in any of the graphs that we cached during setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_graph.plot_attention(random_cache[\"pattern\",0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: In Head 1.6 vertices of low degree attend stongly to vertex position 2. Let's try and work out how the model does this.\n",
    "\n",
    "My first guess as to how this attention pattern in head 1.6 comes about is:\n",
    "- Each vertex of low degree has the concept of \"I am a vertex of low degree\" encoded in its residual stream position. \n",
    "- The query matrix for head 1.6 reads this information from the residual stream and produces queries that encode the concept \"I am looking for the special vertex position\" in low degree vertex positions.\n",
    "- The key matrix for head 1.6 produces a key that says \"I am in the special vertex position\" in position 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 **Patching**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this theory by looking at how earlier components contribute to the attention score in head 1.6.\n",
    "\n",
    "We are going to take a \"corrupted\" graph with (in some sense) no low degree vertices as model input and then \"patch over\" the paths leading from a collection of earlier components to the query and/or the key of head 1.6 using the cached head 0.2 activations for a \"clean\" graph with some low degree vertices. This will allow us to see to what extent we can reproduce the \"clean\" behaviour of head 1.6 using a subset of the components earlier in the model. We can use this to test my theory that degree information is passed into head 1.6 via the query and the positional information pinpointing vertex position 2 is passed in via the key. We should be able to go one further and actually identify which earlier components are responsible for passing this information on to head 1.6. \n",
    "\n",
    "By \"components\" I mean attention heads and the embedding. \n",
    "\n",
    "By \"path\" I mean the sequence of latent variables that lead from a component directly (without involving another component) to another component later in the model (in this case head 1.6).\n",
    "\"Patching over\" a path from component $A$ to component $B$ means effectively replacing all these latent varaibles with what they would be in the hypothetical scenario that component $A$'s output was replaced but all other components outputs stayed the same (even for components that come after $A$ in the model). By doing this we can get a better idea of the extent to which $A$ helps $B$ achieve its function. \n",
    "\n",
    "Notice that I choose not to include the mlps as components. Instead we say that mlps count as part of the direct path between other components. This is because (in the case of this specific circuit) it turns out that all the important paths between other components go through the mlp on layer 0. Insisting on studying paths that skip the mlp obscures important relationships that depend on mlp computation.  \n",
    "\n",
    "Because you can't in general simultaneously patch two different paths during the same forward pass, we implement the above as follows:\n",
    "1. Choose two subsets of the components that come before head 1.6 in the model (i.e. Embed, Layer 1 heads)\n",
    "2. Input a \"corrupted\" graph into the model\n",
    "3. Replace only the output of the components within our first chosen subset with the equivalent output obtained when the \"clean\" graph is the input.\n",
    "4. Continue the model forward pass and cache the query of head 1.6.\n",
    "5. Begin another forward pass on the same corrupted output and this time replace only the output of the components within our *second* chosen subset.\n",
    "6. Continue the forward pass until layer 1 and replace the query in head 1.6 with the cached query from the previous run.\n",
    "7. Observe the resulting attention pattern in head 1.6 and hopefully pinpoint which components are needed to reproduce the \"clean\" attention pattern and in what way they compose with head 1.6.\n",
    "\n",
    "This is a specific implementation of a broader technique known as *path patching*. In the cells below we carry out path patching with the following configuration details:\n",
    "- `q_heads_to_patch` specifies heads $h$ for which the path from $h$ to the query of head 1.6 will be patched.\n",
    "- `q_patch_embed` specifies whether or not to patch the path from the Embed to the query of head 1.6.\n",
    "- `k_heads_to_patch` specifies heads $h$ for which the path from $h$ to the key of head 1.6 will be patched.\n",
    "- `k_patch_embed` specifies whether or not to patch the path from the Embed to the key of head 1.6.\n",
    "\n",
    "Have a play around with this configuration below and see if you can gain some insight that could help with step 7 above.\n",
    "\n",
    "<details>\n",
    "  <summary>Two things to note</summary>\n",
    "  \n",
    "PosEmbed is the same for all inputs so patching it is pointless.\n",
    "\n",
    "The cell below runs path patching for several example configurations and, as such, the four variables described above are actually lists with the ith entry of each forming the configuration for the ith path patching example.\n",
    "  \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupted_input = petersen_graph.A\n",
    "corrupted_cache = petersen_cache\n",
    "clean_input = random_graph.A\n",
    "clean_cache = random_cache\n",
    "\n",
    "layer_1_head = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Components to patch:\n",
    "q_heads_to_patch = [[2,1],[3,4],[1,2,3]]\n",
    "q_patch_embed = [True,True,False]\n",
    "k_heads_to_patch = [[2,3],[2],[1,2,3]]\n",
    "k_patch_embed = [True,False,False]\n",
    " \n",
    "component_lists = [q_heads_to_patch,q_patch_embed,k_heads_to_patch,k_patch_embed]\n",
    "\n",
    "n_examples = len(q_heads_to_patch)\n",
    "\n",
    "assert all(len(lst) == n_examples for lst in component_lists)\n",
    "\n",
    "px.imshow(random_cache[\"pattern\",1][0,6],color_continuous_scale='Blues',title=\"Clean Pattern\",width=1000).show()\n",
    "px.imshow(petersen_cache[\"pattern\",1][0,6],color_continuous_scale='Blues',title=\"Corrupted Pattern\",zmax=1,width=1000).show()\n",
    "\n",
    "for i in range(n_examples):\n",
    "    pattern = get_path_patched_attn_pattern(\n",
    "        model,\n",
    "        corrupted_input,\n",
    "        corrupted_cache,\n",
    "        clean_input,\n",
    "        clean_cache, \n",
    "        q_heads_to_patch = q_heads_to_patch[i],\n",
    "        q_patch_embed = q_patch_embed[i],\n",
    "        k_heads_to_patch = k_heads_to_patch[i],\n",
    "        k_patch_embed = k_patch_embed[i],\n",
    "        layer_1_head = layer_1_head\n",
    "    )\n",
    "\n",
    "    q_embed_string = \" Embed and\" if q_patch_embed[i] else \"\"\n",
    "    k_embed_string = \" Embed and\" if k_patch_embed[i] else \"\"\n",
    "    title = f\"Pattern when the paths from{q_embed_string} heads {q_heads_to_patch[i]} to the query of head 1.{layer_1_head}<br>and the paths from{k_embed_string} heads {k_heads_to_patch[i]} to the key of head 1.{layer_1_head} are patched.\"\n",
    "\n",
    "    px.imshow(\n",
    "        pattern,\n",
    "        color_continuous_scale='Blues',\n",
    "        title=title,\n",
    "        zmax=1,\n",
    "        width=1000\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having played around with the setup above, I made the following observations:\n",
    "1. I cannot reproduce any of the clean pattern purely by patching paths into the key of head 1.6. This suggests that some important information must be entering head 1.6 via the query. This does not in any way rule out my theory that the information pinpointing vertex position 2 enters head 1.6 via the key (but it doesn't support it either).\n",
    "2. Configurations involving patching the path from head 0.2 to the query of head 1.6 tend to be the ones that reproduce the clean pattern most closely. This suggests that head 0.2 is passing important information to head 1.6 via the query. We say that head 0.2 is \"Q-composing\" with head 1.6.\n",
    "3. In configurations that involve head 0.2 but still miss some key parts of the pattern, adding in heads 0.1, 0.3 and/or Embed can help performance. This suggests more than one head is involved in the circuit responsible for the function of head 1.6. It also suggests that Embed has direct effect on head 1.6, not just via layer 0 heads.\n",
    "\n",
    "To be more confident in observations 2 and 3, we can quantify how well the pattern has been reproduced and then repeat the patching process above for all possible configurations and across a whole batch of graphs, recording how well the pattern has been reproduced each time.\n",
    "When I say \"all possible configurations\" I am actually ignoring configurations involving patching paths from layer 0 heads to the key of head 1.6. This to reduce the number of possible configurations to a manageable amount. Based on observation 1, I am confident that this does not missing a configuration that offers a notably good reconstruction of the clean attention pattern.\n",
    "\n",
    "I quantify the pattern reconstuction performance using the `patching_metric` below (scaled so that 1 represents no improvement over the corrupted input and 0 represents perfect reconstruction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patching_metric(\n",
    "    reconstructed_pattern: Float[Tensor,\"batch n_vertices n_vertices\"],\n",
    "    clean_pattern: Float[Tensor,\"batch n_vertices n_vertices\"],\n",
    "    corrupted_pattern: Float[Tensor,\"batch n_vertices n_vertices\"] # batch can be 1 in some cases but broadcasting handles this\n",
    "):\n",
    "    return (torch.linalg.matrix_norm(reconstructed_pattern-clean_pattern)/torch.linalg.matrix_norm(clean_pattern-corrupted_pattern)).mean().item()\n",
    "\n",
    "def get_component_string(results_key):\n",
    "    \"\"\"Given key from results dict, return dict expressing which components that key corresponds to.\"\"\"\n",
    "    q_components = 'Embed '*results_key[0][0] + ' '.join(f'0.{head}' for head in results_key[0][1:])\n",
    "    k_components = 'Embed' if results_key[1][0] else ''\n",
    "    return q_components, k_components\n",
    "\n",
    "def get_n_components_patched(results_key):\n",
    "    \"\"\"Given key from results dict, find total number of components that have been patched\"\"\"\n",
    "    return sum([\n",
    "        results_key[0][0],\n",
    "        results_key[1][0],\n",
    "        len(results_key[0][1:]),\n",
    "        len(results_key[1][1:])\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "results = get_path_patching_metric_results(\n",
    "    model,\n",
    "    corrupted_input,\n",
    "    corrupted_cache,\n",
    "    batch,\n",
    "    batch_cache,\n",
    "    patching_metric,\n",
    "    layer_1_head\n",
    ")\n",
    "   \n",
    "table = Table('Number of components patched','Best Score Achieved','Q Components Patched','K Components Patched',title='Best paths to patch for each number of components patched:')\n",
    "one_component_table = Table(\"Q component\", \"K Component\",\"Pattern Reconstruction Score\", title='Results from patching one component')\n",
    "\n",
    "for n_components_patched in range(1,model.cfg.n_heads+2):\n",
    "    filtered_results = {k:v for k,v in results.items() if get_n_components_patched(k)==n_components_patched}\n",
    "    if n_components_patched==1:\n",
    "        for k,v in filtered_results.items():\n",
    "            q_component, k_component = get_component_string(k)\n",
    "            one_component_table.add_row(q_component, k_component,f\"{v:.2f}\")\n",
    "    best_combination = min(filtered_results, key=filtered_results.get)\n",
    "    best_q_components, best_k_components = get_component_string(best_combination)\n",
    "    best_metric_score = filtered_results[best_combination]\n",
    "    table.add_row(str(n_components_patched),f\"{best_metric_score:.2f}\",best_q_components,best_k_components)\n",
    "\n",
    "rich.print(one_component_table)\n",
    "rich.print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results above make me pretty confident that head 0.2 Q-composing with head 1.6 is the main interaction driving the circuit behind head 1.6's function.\n",
    "\n",
    "Note that we still haven't obtained any evidence supporting my theory that the information pinpointing vertex position 2 enters head 1.6 via the key. While it makes sense the key could have learned to produce a special key vector in position 2 that matches each vector that the query puts in a low degree vertex position, for all we know this information could be passed in via some other less intuitive mechanism. \n",
    "\n",
    "However, a quick check of the activations in the key of head 1.6 (averaged out across a batch of input graphs) strongly suggests that my theory is correct: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = batch_cache['k',1][:,:,6]\n",
    "px.imshow(k.mean(0),color_continuous_scale='RdBu',color_continuous_midpoint=0,title=\"Average key actiavtions for head 1.6\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key clearly knows which position head 1.6 wants to move information from. But how does it know this? From our observations, the choice of position 2 is independent of input and only depends on positional information. Therefore it would make sense if the information comes direct from the PosEmbed. \n",
    "\n",
    "We can test this with some more path patching. This time instead of patching a path using two different input graphs, we are going to patch the direct path from the positional embedding to the key of head 1.6 with the path that we would observe if the positional embeddings for position 2 and position 0 were swapped. We can then see the effect this has on the resulting attention pattern in head 1.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_to_swap_with_2 = 0\n",
    "\n",
    "new_pos_embed = model.W_pos.data.clone()\n",
    "new_pos_embed[[2,position_to_swap_with_2]] = new_pos_embed[[position_to_swap_with_2,2]]\n",
    "\n",
    "pattern = get_pos_embed_to_key_path_patched_attn_pattern(\n",
    "    model,\n",
    "    input=clean_input,\n",
    "    cache=clean_cache,\n",
    "    new_pos_embed=new_pos_embed,\n",
    "    layer_1_head=layer_1_head\n",
    ")\n",
    "\n",
    "title = f\"Pattern when the path from PosEmbed to the key of head 1.{layer_1_head}<br>is patched by switching position 2 and position {position_to_swap_with_2}.\"\n",
    "\n",
    "px.imshow(\n",
    "    pattern,\n",
    "    color_continuous_scale='Blues',\n",
    "    title=title,\n",
    "    zmax=1,\n",
    "    width=1000\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as we expect, we are able to consistently permute the columns of the head 1.6 pattern simply by permuting the rows of the PosEmbed and only allowing this change to effect head 1.6 via a direct path (not via layer 0 heads). This strongly suggests that the only information relevant to pinpointing position 2 in head 1.6 originates in the PosEmbed and enters the head via the key without any information being moved in between postions. Indeed, the mlp between the layer 0 and layer 1 heads acts positionwise so cannot move information and the attention heads are all effectively \"frozen\" by the path patching process.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results give us a pretty good intial overview of the circuit responsible for the function of head 1.6: the information required to pinpoint position 2 passes directly from the PosEmbed into head 1.6 via the key and the degree information passes from head 0.2 to head 1.6 via the query. A few other layer 0 heads also Q-compose with head 1.6 but are less important.\n",
    "<details>\n",
    "    <summary>A diagram to visualise this circuit</summary>\n",
    "\n",
    "The diagram below shows one way to visualise the circuit in the case that vertex 1 has low degree. Each copy of the model architecture represents the action of the model on a single vertex position in the residual stream. Any lines passing between copies represent information being moved between vertex positions. MLPs and irrelevant attention heads are omitted for simplicity. \n",
    "\n",
    "![](diagrams/head16)\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 **Head 0.2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an initial overview, let's follow this trail back through the model by looking more closely at head 0.2. We will again start by studying the attention pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_graph.plot_attention(random_cache['pattern',0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: Vertices of high degree attend more to vertices 3 and 6, whereas vertices of low degree seem to attend evenly to all vertices.\n",
    "\n",
    "Although this pattern seems conceptually similar to head 1.6, there are three main differences that I observe:\n",
    "1. Instead of vertex position 2, vertices attend to vertex positions 3 and 6.\n",
    "2. Instead of low degree vertices, it is the high degree vertices that attend strongly to the special vertex positions.\n",
    "3. Instead of seemingly all \"low degree\" vertices attending maximally to the special vetex position (like in head 1.6), here it seems like attention scales with degree: the higher a vertex's degree, the more strongly it attends to vertex positions 3 and 6.\n",
    "\n",
    "Based on these observations, I have a similar theory for this heads mechansim as I did for head 1.6: the query receives the degree information and the key receives the positional information required for pinpointing positions 3 and 6.\n",
    "\n",
    "As before, we will now test this theory. However, now that we are studying a layer 0 head we have two simplifications that we can take advantage of:\n",
    "1. There are only two inputs to this head: the Embed and PosEmbed.\n",
    "2. There are no non-linear mlps between us and the model input. \n",
    "\n",
    "This means we can use some more straight forward techniques to study how the Embed and PosEmbed interact with head 0.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $X$ be an adjacnecy matrix model input, let $W_Q^h$ and $W_K^h$ be the query and key matrices for head $h$ and let $W_{pos}$ and $W_E$ be the positional embedding and embedding matrices. Then the query $q$ and key $k$ of head 0.2 are given by:\n",
    "\n",
    "$$q = XW_EW_Q^{0.2}+ W_{pos}W_Q^{0.2}$$\n",
    "\n",
    "$$k = XW_EW_K^{0.2}+ W_{pos}W_K^{0.2}$$\n",
    "\n",
    "This means we can nicely decompose $q$ and $k$ into their two terms and quantify how much each one contributes to the result. We will do this by taking the norm of each component of the sum and averaging out across a batch of inputs: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposed_qk_input_layer_0 = decompose_qk_input(model, batch_cache, layer=0)\n",
    "decomposed_q_layer_0 = decompose_q(model,decomposed_qk_input_layer_0,head_index=2,layer=0)\n",
    "decomposed_k_layer_0 = decompose_k(model,decomposed_qk_input_layer_0,head_index=2,layer=0)\n",
    "\n",
    "component_labels_layer_0 = [\"Embed\", \"PosEmbed\"]\n",
    "for decomposed_input, name in [(decomposed_q_layer_0, \"query\"), (decomposed_k_layer_0, \"key\")]:\n",
    "    px.imshow(\n",
    "        decomposed_input.pow(2).sum(-1).sqrt().mean(0).detach(),\n",
    "        labels={\"x\": \"Position\", \"y\": \"Component\"},\n",
    "        title=f\"Norms of components of {name}\",\n",
    "        y=component_labels_layer_0,\n",
    "        color_continuous_scale='Blues',\n",
    "        width=1000, height=400,\n",
    "        zmin=0\n",
    "    ).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, these plots suggest that all the degree information enters head 0.2 via the query. Indeed, degree information can only come from the Embed and when averaged out across a whole batch, we expect this information to be even distributed across vertex positions. This matches what we observe in the plots above: specifically a solid dark blue stripe across the Embed row in the first plot as well as a very light blue strip in the same place in the second plot, suggesting that the key cannot be using much degree information. \n",
    "\n",
    "These plots also support my theory that the positional information pinpointing vertex positions 3 and 6 enters via the key. Indeed the second plot suggests that the information in these postions in the PosEmbed is consistently of particular importance to the key but the query seems to give similar importance to all positions in PosEmbed.\n",
    "\n",
    "One additional observation of interest is that, as well as the degree information from the Embed, it seems that there is quite a lot of information from the PosEmbed entering head 0.2 via the query (suggested by the blue stripe across the PosEmbed row of the first plot). While this does not contradict any of my theories, it does suggest that there could be something more happening in head 0.2. Similarly, the are positions other than 3 and 6 in the key with reasonably sized PosEmbed component norms, also suggesting that there could be a more subtle purpose to head 0.2 as well.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One problem with the visualisations above is that, while it believable that it is correlated with the importance of a component, the norm obscures a lot of the detail when it comes to how the PosEmbed and Embed effect head 0.2. We can get a better idea of their effect by recalling that the attention scores for head 0.2 are given by $qk^{T}/\\sqrt{d_{head}}$. This means we can use our decompositions of $q$ and $k$ from before to decompose the attention scores into four terms corrseponding to the contribution of each pairing of the two query components and the two key components to the overall attention score. This gives us a clearer idea of the impact each pairing of components has.\n",
    "\n",
    "For this visualisation we have to specifiy a single graph input with `batch_sample_idx` as opposed to avergaing over a batch. Indeed, averaging over the norms makes some sense but attention scores are very input dependent so averaging over them would just blur a lot of detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposed_scores_layer_0 = decompose_attn_scores(decomposed_q_layer_0, decomposed_k_layer_0)\n",
    "\n",
    "plot_decomposed_attn_scores(\n",
    "    decomposed_scores=decomposed_scores_layer_0,\n",
    "    cache=batch_cache,\n",
    "    component_labels=component_labels_layer_0,\n",
    "    layer=0,\n",
    "    head_idx=2,\n",
    "    batch_sample_idx=100,\n",
    "    zmax=3.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These plots offer much more support to my theory. The most important interactions for creating the attention pattern in head 0.2 are clearly the degree information passing from the Embed to the head via the query and the positional information passing from the PosEmbed to the head via the key. \n",
    "\n",
    "However, as we suspected, this is not the whole story. In particular, it seems that the query uses information from the PosEmbed to uniformly subtract from the attention scores in positions 3 and 6 (shown in the bottom right of the grid of four plots above). If the head's purpose is simply to allow vertices to attend to postions 3 and 6 based on their degree (in line with our observations), then this uniform subtraction seems unecessary. Is this just a quirk of the model or does it represent some key computation with an as yet unkown puropse? We will explore this more in the next section.   \n",
    "\n",
    "<details>\n",
    "    <summary>A diagram to visualise this part of the circuit</summary>\n",
    "    \n",
    "Just as we did earlier, we can visualise this part of the circuit, this time in the case that vertex 4 has high degree. Here we denote the contribution from the PosEmbed to the query with a lighter shade of green.\n",
    "\n",
    "![](diagrams/head02)\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. **Zooming in**: following the circuit step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a broad overview of which components are composing to form the circuit responsible for head 1.6's function, it is time to attempt to follow this circuit through the network from input to head 1.6 in more detail.\n",
    "\n",
    "Hopefully by doing this we can answer questions like:\n",
    "- How exactly does $W_Q^{0.2}$ give the query vectors a concept of degree based on $XW_E$?\n",
    "- What does the mlp layer do to the information moving between heads 0.2 and 1.6?\n",
    "\n",
    "We begin by trying to answer the first of these questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this section we focus on one single input which we set below. I encourage you to swap this out for another graph that we cached during the setup in order to see how the circuit behaves for different inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = random_graph\n",
    "cache = random_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 **Input -> Head 0.2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the query vectors are given by:\n",
    "\n",
    "$q = XW_EW_Q^{0.2}+ W_{pos}W_Q^{0.2}$,\n",
    "\n",
    "and that multiplying by $X$ is just summing the rows of $W_EW_Q^{0.2}$ corresponding to the vertices adjacent to each vertex. \n",
    "\n",
    "This means that studying the rows of $W_EW_Q^{0.2}$ and $W_{pos}W_Q^{0.2}$ will help us decompose what $W_Q^{0.2}$ does into smaller parts instead of looking at what it does to the entire residual stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 0\n",
    "head = 2\n",
    "\n",
    "W_E = model.W_E.detach()\n",
    "W_pos = model.W_pos.detach()\n",
    "W_Q = model.W_Q[layer,head].detach()\n",
    "W_K = model.W_K[layer,head].detach()\n",
    "resid = cache['resid_pre',0][0]\n",
    "\n",
    "graph.plot()\n",
    "px.imshow(W_E,color_continuous_scale='RdBu',title='W_E',width=1500).show()\n",
    "px.imshow(W_pos,color_continuous_scale='RdBu',title='W_pos',width=1500).show()\n",
    "px.imshow(W_Q,color_continuous_scale='RdBu',title=f'W_Q (head 0.{head})',width=1500).show()\n",
    "px.imshow(W_E@W_Q,color_continuous_scale='RdBu',title=f'W_E @ W_Q (head 0.{head})',width=1500,zmax=0.5,zmin=-0.5).show()\n",
    "px.imshow(W_pos@W_Q,color_continuous_scale='RdBu',title=f'W_pos @ W_Q (head 0.{head})',width=1500,zmax=0.5,zmin=-0.5).show()\n",
    "px.imshow(resid@W_Q,color_continuous_scale='RdBu',title=f'q (= resid @ W_Q) (head 0.{head})',width=1500).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these plots we can observe that, given a vertex in position $i$ with degree $d_i$, $W_Q^{0.2}$ maps the embedding of all $d_i$ adjacent vertices (found in the rows of $W_E$) to pretty much the same vector $\\mathbf{v}$ (represented by the verticle stripes in the plot of $W_EW_Q^{0.2}$ above). It also maps the positional embedding of $i$ (found in the rows of $W_{pos}$) to $-2\\mathbf{v}$. As described earlier, multiplying by $X$ sums the results in the rows of $W_EW_Q^{0.2}$ meaning that row $i$ of $XW_EW_Q^{0.2}$ contains the vector $d_i\\mathbf{v}$. By then summing the results with $W_{pos}W_Q^{0.2}$ we are subtracting $2\\mathbf{v}$ meaning the query of head 0.2 has the information $d_i-2$ stored in each vertex position $i$. \n",
    "\n",
    "By identifying this direction in the latent space of head 0.2's QK circuit corresponding to the concept of degree, we can read off the degree of each vertex directly from the query $q$ plotted above.  \n",
    "\n",
    "Given this insight, we now expect the key vectors in positions 3 and 6 of head 0.2 to also point in the direction $\\mathbf{v}$, therefore causing vertices with higher degree to attend more strongly to those positions. This means that the $W_{pos}W_K^{0.2}$ should make up the majority of the key $k$ (in line with the earlier decomposition analysis). Sure enough, when we plot $W_EW_K^{0.2}$ and $W_{pos}W_K^{0.2}$, our expectations are met: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.imshow(W_K,color_continuous_scale='RdBu',title=f'W_K (head 0.{head})',width=1500).show()\n",
    "px.imshow(W_E@W_K,color_continuous_scale='RdBu',title=f'W_E @ W_K (head 0.{head})',width=1500,color_continuous_midpoint=0,zmax=1.25,zmin=-1.25).show()\n",
    "px.imshow(W_pos@W_K,color_continuous_scale='RdBu',title=f'W_pos @ W_K (head 0.{head})',width=1500,color_continuous_midpoint=0,zmax=1.25).show()\n",
    "px.imshow(resid@W_K,color_continuous_scale='RdBu',title=f'k (= resid @ W_K) (head 0.{head})',width=1500).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 **Head 0.2 -> Head 1.6**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having reverse engineered the computation behind head 0.2's attention pattern, we now look at how that pattern is used to write information out to the residual stream. If $W_V^h$ is the value matrix for head $h$, then $W_V^{0.2}$ essentially appears to be doing that same thing as $W_K^{0.2}$, except with a different direction corresponding to degree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_V = model.W_V[layer,head].detach()\n",
    "z = cache['z',layer][0,:,head]\n",
    "result = cache['result',layer][0,:,head]\n",
    "\n",
    "px.imshow(W_V,color_continuous_scale='RdBu',title=f'W_V (head 0.{head})',color_continuous_midpoint=0).show()\n",
    "px.imshow(W_E@W_V,color_continuous_scale='RdBu',title=f'W_E @ W_V (head 0.{head})',color_continuous_midpoint=0).show()\n",
    "px.imshow(W_pos@W_V,color_continuous_scale='RdBu',title=f'W_pos @ W_V (head 0.{head})',color_continuous_midpoint=0).show()\n",
    "px.imshow(resid@W_V,color_continuous_scale='RdBu',title=f'v (= resid @ W_V) (head 0.{head})',color_continuous_midpoint=0).show()\n",
    "px.imshow(z,color_continuous_scale='RdBu',title=f'z (attention pattern applied to v) (head 0.{head})',color_continuous_midpoint=0).show()\n",
    "px.imshow(result,color_continuous_scale='RdBu',title=f'result (output of head 0.{head})',color_continuous_midpoint=0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these plots we can see how the value has the attention pattern applied to it (see z plot) and then is written to the residual stream (see result plot). In every plot we can see similar patterns. This because every row encodes degree in the same direction in the corresponding latent space. We now have a good understanding of how head 0.2 writes degree information to the residual stream for head 1.6 to read and use for its task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Between heads 0.2 and 1.6 we have an mlp layer, which makes it hard to track what happens to the output of a specific head. Indeed, because the mlp is non-linear we can't decompose the post mlp residual stream into nice components like we did for the residual stream input into the layer 0 attention heads.\n",
    "In the general case, just because a layer 0 head outputs a vector in the residual stream pointing in one direction does not mean this vector will be available to be read by a layer 1 head, the mlp can easily mess it up. \n",
    "\n",
    "This is a problem for head 1.6. As we have seen above, head 0.2 (among others) has written a result to the residual stream that contains the degree of each vertex as a scaled version of the same vector in each vertex position. In other words, we have an approximate direction in the residual stream that corresponds to the degree of a vertex. For head 1.6's purposes, it would be nice if the mlp did not tamper with this direction at all. Unfotunately, the mlp has lots of other jobs to do and so this nice interpretable direction appears to be lost. To visualise this, we plot two things below:\n",
    "1. The output obtained when you pass the output of head 0.2 ('result' plotted above) through the mlp. You can think of this as the \"theortecial output of the mlp if the output of head 0.2 was the only information the model wanted to pass to the next layer\".\n",
    "2. The full residual stream after the mlp on layer 0 below.\n",
    "\n",
    "Compare these plots with the 'result' plot above to see the effect the mlp's non-linearity has on one head's output vs when it has to handle a full residual streams worth of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_0 = model.blocks[0].mlp\n",
    "\n",
    "post_mlp = (mlp_0(result)+result).detach()\n",
    "\n",
    "\n",
    "px.imshow(post_mlp,color_continuous_scale='RdBu',title=\" Theoretical/fake 'component' of post mlp 0 residual stream orignating from head 0.2\").show()\n",
    "px.imshow(cache['resid_post',0][0],color_continuous_scale='RdBu',title='Post mlp 0 residual stream').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"theoretical\" plot shows that the mlp seems to have a linear effect on the output of head 0.2 (the degree direction is rotated and scaled but the relative sizes of the vector pointing in the degree direction in each vertex position remain the same). We can still easily \"read off\" the degree information from the output of the mlp. Unfotunately, the actual output of the mlp (in the next plot) suggests that in general we probably wont have one residual direction from which to \"read off\" information like a vertex's degree. The actual residual stream appears to be very noisy.\n",
    "\n",
    "However, in this specific case, I can't ignore the similarity between the entries in the 68th basis direction ($e_{68}$) of the residual stream across all of the last three plots. It might just be a coincidence, but the pattern in that column matches the degree information we want to pass on (low values corresponding to high degree vertices and vice versa). Have a go at switching the input graph at the start of this section and seeing if you can replicate this observation in other cases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this could well be an interesting observation, it only has any relevance to head 1.6 if it actually takes adavntage of this apparent preservation of the degree direction by reading information from $\\langle e_{68}\\rangle$ (the subspace of the residual stream spanned by $e_{68}$). Let's plot $W_Q^{1.6}$ to get a better idea of where the query information is read in from (we will plot its transpose for easier visualisation, but remember it has shape (128, 32)): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_Q_1_6 = model.W_Q[1,6].detach()\n",
    "\n",
    "px.imshow(W_Q_1_6.T,color_continuous_scale='RdBu',title='W_Q (head 1.6)').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot makes me pretty confident that what we observed before is not a coincidence and that the non-linear mlp is preserving the degree pattern in a linear way by rotating and scaling it to roughly line up with $e_{68}$ in the residula stream.\n",
    "\n",
    "We can gather more evidence for this by taking a batch of input graphs and, for each graph, doing the following:\n",
    "1. Consider the output of head 0.2 and find the residual direction corresponding to degree that this head writes to.\n",
    "2. For each vertex find the coefficient of this direction vector that scales the vector, encoding the degree of the vertex.\n",
    "3. Form the 10 dimensional vector of these coefficients. This can be thought of as the \"degree signature\" of that specific graph and can be clearly visualised by looking at the columns of the output of head 0.2 ('result' plot above).\n",
    "4. Record the cosine similarity between the degree signature and each column of the post mlp residual stream.\n",
    "\n",
    "Once completed for each graph, average these results across the whole batch.\n",
    "\n",
    "These cosine similarities tell us the extent to which the relative sizes of the vectors in the \"degree direction\" pre mlp match the relative sizes of the vectors in $\\langle e_i\\rangle$ post mlp (for each $i$ in $[0,...,127]$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_degree_signature_resid_basis_cossim(pre_mlp,post_mlp):\n",
    "    return einops.einsum(\n",
    "        pre_mlp/pre_mlp.norm(dim=1).unsqueeze(1),post_mlp/post_mlp.norm(dim=1).unsqueeze(1),\n",
    "        \"batch n_vertices d_model, batch n_vertices d_model -> d_model\"\n",
    "    ).detach()/pre_mlp.shape[0]\n",
    "\n",
    "post_mlp = batch_cache['resid_pre',1]\n",
    "head_0_2_output_pre_mlp = batch_cache['result',0][:,:,2]\n",
    "\n",
    "cossims = get_degree_signature_resid_basis_cossim(head_0_2_output_pre_mlp,post_mlp)\n",
    "\n",
    "px.line(\n",
    "    y=cossims,\n",
    "    labels={\"x\": \"Resid stream basis direction\", \"y\": \"Degree signature cosine similarity\"},\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot shows us that the information held in $\\langle e_{68}\\rangle$ is, on average, very similar to the degree signature of the input graph. From this I conclude that the degree information is passed from head 0.2 to the query of head 1.6 via the mlp which rotates the \"degree direction\" in the residual stream to allign with $e_{68}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now follow this degree information into head 1.6. We can observe how $W_Q^{1.6}$ reads the degree information from the residual stream to create a query $q$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.imshow(cache['resid_pre',1][0]@W_Q_1_6,color_continuous_scale='RdBu',title='q (= resid @ W_Q) (head 1.6)',width=1500).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to head 0.2, head 1.6 clearly has a direction corresponding to degree (shown by the three similar stripes across the rows corresponding to low degree vertices). But unlike head 0.2 it seems like this direction *only* appears in the rows corresponding to low degree vertices. Suppressing the high degree vertices like this cannot be achieved only from the linear action of $W_Q^{1.6}$ on the degree signature in $\\langle e_{68}\\rangle$ so $W_Q^{1.6}$ must be reading additional information in from the residual stream. My best guess from studying $W_Q^{1.6}$ is that this information comes from $\\langle e_{80}\\rangle$ but I am yet to gather any other evidence to support this.\n",
    "\n",
    "However $W_Q^{1.6}$ does this suppression, we can now clearly see the vertices that head 1.6 wants to move information to, right there in the query. All that's left is to observe how the key creates a vector in position 2 that matches this \"low degree vertex\" direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike $W_Q^{1.6}$, $W_K^{1.6}$ does not appear to be reading from a small set basis directions in the residual stream:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_K_1_6 = model.W_K[1,6].detach()\n",
    "\n",
    "px.imshow(W_K_1_6.T,color_continuous_scale='RdBu',title='W_K (head 1.6)').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes is more difficult to theorise exactly what the mlp is doing to the information that is then read into head 1.6 by $W_K^{1.6}$. However, as we saw in section 1, we know that this information predominantly comes directly from the PosEmbed and is used to create a key vector in position 2 that matches the query vectors in low degree vertex positions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.imshow(cache['resid_pre',1][0]@W_K_1_6,color_continuous_scale='RdBu',title='q (= resid @ W_K)',width=1500).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that, I think we have done enough to say that we have reverse engineeredi a large part of the circuit behind the function of head 1.6, we even followed the input step by step through each layer and identified the directions in each latent space that corresponded to degree. There are, of course, contributions to the input to head 1.6 that come from heads other than 0.2 that we have not considered in as much detail as head 0.2. I will discuss these in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. **Next Steps**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The obvious next question is: \"what does any of this have to do with the planarity of a graph?\". I am not sure at all. My best guess so far is that the model could be leveraging the fact that *vertices of degree less than 3 are not relevant to planarity*. Indeed for vertices of degree 0 or 1, removing them from a graph is never going to change a graph's planarity. For vertices of degree 2, replacing them with a single edge between their adjacent vertices is never going to change a graph’s planarity. Perhaps the purpose of head 1.6 is to help the model ignore this redundant information when making its decision? If true, it could help explain the significance of $W_{pos}W_Q^{0.2}$ representing “-2 in the degree direction”: it could be creating a “cutoff” below which all vertices should be in some sense ignored.\n",
    "\n",
    "To add further speculation to this theory, note the following:\n",
    "\n",
    "While most of the time head 1.6 seems to pick out only vertices of degree 2 or less, there are some graphs for which vertices with higher degree also receive information from position 2.\n",
    "\n",
    "Here is an example (look at vertex 9):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.tensor([[\n",
    "    [0., 1., 1., 0., 1., 1., 0., 0., 0., 1.],\n",
    "    [1., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
    "    [1., 0., 0., 0., 1., 1., 1., 0., 1., 0.],\n",
    "    [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
    "    [1., 0., 1., 0., 0., 0., 0., 0., 0., 1.],\n",
    "    [1., 1., 1., 0., 0., 0., 0., 1., 1., 0.],\n",
    "    [0., 0., 1., 0., 0., 0., 0., 0., 1., 0.],\n",
    "    [0., 0., 0., 0., 0., 1., 0., 0., 1., 0.],\n",
    "    [0., 0., 1., 0., 0., 1., 1., 1., 0., 0.],\n",
    "    [1., 0., 0., 1., 1., 0., 0., 0., 0., 0.]\n",
    "]])\n",
    "\n",
    "exception_graph = Graph(A)\n",
    "\n",
    "model.set_use_attn_result(True)\n",
    "_, exception_cache = model.run_with_cache(exception_graph.A)\n",
    "model.set_use_attn_result(False)\n",
    "\n",
    "exception_graph.plot_attention(exception_cache['pattern',1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does vertex 9 attend strongly to position 2 when it has degree 3? A clue is to look at the attention pattern when we patch the path from head 0.2 to the query of head 1.6 (like we did in section 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Components to patch:\n",
    "q_heads_to_patch = [2]\n",
    "\n",
    "px.imshow(exception_cache[\"pattern\",1][0,6],color_continuous_scale='Blues',title=\"Clean Pattern\",width=1000).show()\n",
    "px.imshow(petersen_cache[\"pattern\",1][0,6],color_continuous_scale='Blues',title=\"Corrupted Pattern\",zmax=1,width=1000).show()\n",
    "\n",
    "\n",
    "pattern = get_path_patched_attn_pattern(\n",
    "    model,\n",
    "    petersen_graph.A,\n",
    "    petersen_cache,\n",
    "    exception_graph.A,\n",
    "    exception_cache, \n",
    "    q_heads_to_patch, \n",
    "    q_patch_embed=False, \n",
    "    k_heads_to_patch=[], \n",
    "    k_patch_embed=False, \n",
    "    layer_1_head=6\n",
    ")\n",
    "    \n",
    "q_embed_string = \" Embed and\" if q_patch_embed else \"\"\n",
    "k_embed_string = \" Embed and\" if k_patch_embed else \"\"\n",
    "title = f\"Pattern when the path from heads {q_heads_to_patch} to the query of head 1.6 are patched.\"\n",
    "\n",
    "px.imshow(\n",
    "    pattern,\n",
    "    color_continuous_scale='Blues',\n",
    "    title=title,\n",
    "    zmax=1,\n",
    "    width=1000\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Head 0.2 manages help head 1.6 to detect all the vertices of degree 2 or below. This is in line with what we expect, given that we observed how head 0.2 effectively calculates the degree of each vertex and subtracts 2. However, with head 0.2 alone, head 1.6 cannot detect vertex 9. Experimenting with patching in different heads above shows us that the computation required in the layer 0 heads in order for head 1.6 to detect vertex 9 is much more nuanced and requires several heads working together. To reverse engineer this would require much more analysis.\n",
    "\n",
    "The next question is \"*why* does vertex 9 attend strongly to position 2 when it has degree 3?\" This is seemingly in contradiction to our speculation that head 1.6 is helping the model \"ignore vertices of degree less than 3 because they dont effect planarity\". If vertex 9 doesn't have degree less than 3 then why would the model want to ignore it? My theory is that the model has calculated that vertex 9 has degree less than 4 and has a neighbour that has degree less than 2. This means that vertex 9 also has no effect on the planarity of the graph, and so can be ignored. Whether this theory is true or not, it is clear from this example that there is more to discover about head 1.6.\n",
    "\n",
    "The next obvious steps are to:\n",
    "- Carry out analysis on other layer 0 heads similar to the analysis we did on head 0.2.\n",
    "- Try and better understand how the output of head 1.6 is used later in the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. **Conclusion** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While I haven't"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unused Stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 **Decompose input**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this theory by looking at how earlier components contribute to the query and key in head 1.6.\n",
    "\n",
    "To do this we take the input into head 1.6 (i.e. the residual stream post layer 0, $x_1$) and think of it as a sum of the outputs of all the previous components 10 components $y_i$ (i.e. the 2 embeddings and 8 layer 0 heads) which is then fed through the mlp on layer 0 $f_0$:\n",
    "$$\n",
    "x_1 = f_0\\left(\\sum_{i = 0}^9 y_i\\right) + \\sum_{i = 0}^9 y_i = f_0\\left(gW_E + W_{pos} + \\sum_{h}A^h x_0 W_{OV}^h\\right) + gW_E + W_{pos} + \\sum_{h}A^h x_0 W_{OV}^h\n",
    "$$\n",
    "Decomposing $x_1$ into each of its terms allows us to study the impact each component has on each position in the query $q$ and key $k$ of head 1.6:\n",
    "$$\n",
    "q = \\left(\\sum_{i = 0}^9 y_i\\right)W_Q^{1.6} = \\sum_{i = 0}^9 y_iW_Q^{1.6} = gW_EW_Q^{1.6} + W_{pos}W_Q^{1.6} + \\sum_{h}A^h x_0 W_{OV}^hW_Q^{1.6}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_index = 6\n",
    "\n",
    "decomposed_qk_input = decompose_qk_input(model,batch_cache)\n",
    "\n",
    "actual_resid = batch_cache['resid_post',0][0]\n",
    "naive_resid = decomposed_qk_input.sum(1)[0].detach()\n",
    "\n",
    "px.imshow(actual_resid,color_continuous_scale='RdBu',zmax=3.5,zmin=-3.5).show()\n",
    "px.imshow(naive_resid,color_continuous_scale='RdBu',zmax=4.5,zmin=-4.5).show()\n",
    "px.imshow(actual_resid-naive_resid,color_continuous_scale='RdBu',zmax=4.5,zmin=-4.5).show()\n",
    "px.imshow(model.W_Q[1,6].detach(),color_continuous_scale='RdBu').show()\n",
    "\n",
    "decomposed_q = decompose_q(model, decomposed_qk_input, head_index)\n",
    "decomposed_k = decompose_k(model, decomposed_qk_input, head_index)\n",
    "\n",
    "component_labels = [\"Embed\", \"PosEmbed\"] + [f\"0.{h}\" for h in range(model.cfg.n_heads)]\n",
    "for decomposed_input, name in [(decomposed_q, \"query\"), (decomposed_k, \"key\")]:\n",
    "    px.imshow(\n",
    "        decomposed_input.pow(2).sum(-1).sqrt().mean(0).detach(),\n",
    "        labels={\"x\": \"Position\", \"y\": \"Component\"},\n",
    "        title=f\"Norms of components of {name}\",\n",
    "        y=component_labels,\n",
    "        color_continuous_scale='Blues',\n",
    "        width=1000, height=400\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we believe that the Norms of the components of the query and the key of head 1.6 are correlated with the importance of that component in the function of head1.6, then these plots support my theory that any degree information is passed into head 1.6 via Q-composition from components earlier in the model.\n",
    "Indeed if it was passed via K-composition then we would expect to see components of the key which have high norms across all positions, as low degree vertices can occur at any position.\n",
    "\n",
    "**Further Observations**: \n",
    "- We also see that when creating keys, head 1.6 pretty much only cares about the positional information coming from position 2. This suggests that the concept of \"I want to gather low degree vertices\"&mdash;that is encoded in the position 2 key&mdash;comes entirely from head 1.6 and is not passed on from an earlier component.\n",
    "- The query plot suggests that, out of the previous components, Embed, PosEmbed and head 0.2, are primarily responsible for passing on degree infomation to the vertex positions in head 1.6, with heads 0.1 and 0.3 playing a less important role.\n",
    "\n",
    "These plots are not fully convincing though, as the norm is not obviously a principled indicator of causality from earlier components to later ones. We can get a better idea of which components are causaly important for the function of head 1.6 by taking a single example graph and pairing up individual key and query components and seeing what attention scores they produce:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposed_scores = decompose_attn_scores(decomposed_q, decomposed_k)\n",
    "\n",
    "plot_decomposed_attn_scores(\n",
    "    decomposed_scores=decomposed_scores,\n",
    "    cache=batch_cache,\n",
    "    component_labels=component_labels,\n",
    "    head_idx=6,\n",
    "    batch_sample_idx=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This strongly supports my theory: it would be a pretty big coincidence if the output space of head 0.2 happend to be the only layer 0 head to have a strong overlap with the query input space of head 1.6 and head 0.2 wasn't integral to head 1.6's functionality. Similarly, we see that the output space of PosEmbed has a strong overlap with the key input space of head 1.6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 **Composition scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head_idx = 2\n",
    "out = (mlp_0(W_OV[0]) + W_OV[0]).detach()\n",
    "px.imshow(out[head_idx],color_continuous_scale='RdBu').show()\n",
    "px.imshow(mlp_0(W_OV[0])[head_idx].detach(),color_continuous_scale='RdBu').show()\n",
    "px.imshow(W_OV[0,head_idx].detach(),color_continuous_scale='RdBu').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all QK and OV matrices\n",
    "W_QK = model.W_Q @ model.W_K.transpose(-2, -1)\n",
    "W_OV = model.W_V @ model.W_O\n",
    "\n",
    "mlp_0 = model.blocks[0].mlp\n",
    "\n",
    "# note: adding embed and pos_embed to output spaces may not be very principled (it was my idea and I don't know if anyone else does it)\n",
    "output_space = [\n",
    "    (mlp_0(model.W_E) + model.W_E).unsqueeze(dim=0),\n",
    "    (mlp_0(model.W_pos) + model.W_pos).unsqueeze(dim=0),\n",
    "    *list(mlp_0(W_OV[0]) + W_OV[0])\n",
    "]\n",
    "\n",
    "input_space = {\n",
    "    'Q': W_QK[1],\n",
    "    'K': W_QK[1].transpose(-2,-1),\n",
    "    'V': W_OV[1]\n",
    "}\n",
    "\n",
    "# Define tensors to hold the composition scores\n",
    "composition_scores = {\n",
    "    \"Q\": torch.zeros(model.cfg.n_heads+2, model.cfg.n_heads).to(device),\n",
    "    \"K\": torch.zeros(model.cfg.n_heads+2, model.cfg.n_heads).to(device),\n",
    "    \"V\": torch.zeros(model.cfg.n_heads+2, model.cfg.n_heads).to(device),\n",
    "}\n",
    "\n",
    "for i,out_sp in enumerate(output_space):\n",
    "    for j in range(model.cfg.n_heads):\n",
    "        for comp_type in \"QKV\":\n",
    "            composition_scores[comp_type][i, j] = get_comp_score(out_sp, input_space[comp_type][j])\n",
    "\n",
    "# baseline\n",
    "n_samples = 300\n",
    "comp_scores_baseline = np.zeros(n_samples)\n",
    "for i in tqdm(range(n_samples)):\n",
    "    comp_scores_baseline[i] = generate_single_random_comp_score(model)\n",
    "\n",
    "# px.histogram(\n",
    "#     comp_scores_baseline,\n",
    "#     nbins=50,\n",
    "#     width=800,\n",
    "#     labels={\"x\": \"Composition score\"},\n",
    "#     title=\"Random composition scores\"\n",
    "# )\n",
    "\n",
    "for comp_type in \"QKV\":\n",
    "    plot_comp_scores(\n",
    "        model,\n",
    "        composition_scores[comp_type], \n",
    "        title=f\"{comp_type} Composition Scores\",\n",
    "        component_labels=component_labels,\n",
    "        baseline=comp_scores_baseline.mean()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **OV circuit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_OV_circuit(model,layer,head):\n",
    "    W_O = model.W_O[layer, head]\n",
    "    W_V = model.W_V[layer, head]\n",
    "    W_E = model.W_E\n",
    "    W_U = model.W_U\n",
    "\n",
    "    OV_circuit = FactoredMatrix(W_V, W_O)\n",
    "    full_OV_circuit = W_E @ OV_circuit @ W_U\n",
    "    return full_OV_circuit, OV_circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zmax = 0.4\n",
    "\n",
    "layers = list(range(model.cfg.n_layers))\n",
    "heads = list(range(model.cfg.n_heads))\n",
    "fig = make_subplots(\n",
    "    rows=2, \n",
    "    row_titles=[str(i) for i in layers],\n",
    "    cols=8,\n",
    "    column_titles=[str(i) for i in heads]\n",
    ")\n",
    "\n",
    "for layer in layers:\n",
    "    for head_index in heads:\n",
    "        full_OV_circuit, OV_circuit = get_OV_circuit(model,layer,head_index)\n",
    "        heatmap = go.Heatmap(z=OV_circuit.AB.detach(),zmin=-zmax,zmax=zmax,colorscale='RdBu')\n",
    "        fig.add_trace(heatmap, row=layer+1, col=head_index+1)\n",
    "        fig.update_yaxes(autorange='reversed')\n",
    "\n",
    "# Update the layout for better visualization\n",
    "fig.update_layout(height=600, width=2000, showlegend=False)\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.imshow(\n",
    "    get_OV_circuit(model,0,2)[1].AB.detach(),\n",
    "    color_continuous_scale=\"RdBu\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Visualise neurons**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acts(model,cache):\n",
    "    \"\"\"\n",
    "    Plots mean neuron activations for all mlp layers in cache.\n",
    "    \"\"\" \n",
    "    assert len(cache['embed'].shape) > 2, \"No batch dimension found.\"\n",
    "    for i in range(model.cfg.n_layers):\n",
    "        px.imshow(\n",
    "            cache[\"mlp_post\",i].mean(0), # mean over batch dimension\n",
    "            y=[f\"0.{h}\" for h in range(model.cfg.n_vertices)],\n",
    "            labels={\"x\": \"Neurons\", \"y\": \"Vertices\"},\n",
    "            title=f\"Layer {i} Neuron Activations\",\n",
    "            color_continuous_scale=\"Blues\"\n",
    "        ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acts(model,batch_cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Correlation between neuron activation and vertex degree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_degree_act_correlation(\n",
    "    input: Float[Tensor,\"batch n_vertices n_vertices\"],\n",
    "    acts: Float[Tensor,\"batch n_vertices d_mlp\"]\n",
    "):\n",
    "    degrees = input.sum(dim=-1) # batch n_vertices\n",
    "    degrees_flattened = einops.rearrange(\n",
    "        degrees,\n",
    "        \"batch n_vertices -> 1 (batch n_vertices)\"\n",
    "    )\n",
    "\n",
    "    acts_flattened = einops.rearrange(\n",
    "        acts,\n",
    "        \"batch n_vertices d_mlp-> d_mlp (batch n_vertices)\"\n",
    "    )\n",
    "\n",
    "    degree_acts_stack = torch.cat([degrees_flattened,acts_flattened]) # (batch n_vertices) d_mlp+1\n",
    "    print(degree_acts_stack.shape)\n",
    "    correlations = torch.corrcoef(degree_acts_stack)[0,1:] # d_mlp\n",
    "\n",
    "    return correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = get_dataloader('adj','data/n10_15-21_tn_vn/val.npz',128)\n",
    "input, _ = next(iter(loader))\n",
    "_, cache = model.run_with_cache(input)\n",
    "degree_act_correlations = get_degree_act_correlation(input, cache['resid_pre',1])\n",
    "px.line(\n",
    "    y=degree_act_correlations.detach(),\n",
    "    labels={\"x\": \"Neuron\", \"y\": \"Correlation with degree\"},\n",
    "    title=f\"Correlation between neuron activation and degree\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Logit attribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_results, component_captions = batch_cache.get_full_resid_decomposition(return_labels=True, expand_neurons=False)\n",
    "layer_results = torch.stack([v for k,v in batch_cache.items() if 'out' in k])\n",
    "layer_captions = [k for k in cache if 'out' in k]\n",
    "\n",
    "logit_diff_directions = get_logit_diff_directions(model,batch_labels)\n",
    "\n",
    "component_logit_attribution = get_logit_attribution(component_results,logit_diff_directions)\n",
    "layer_logit_attribution = get_logit_attribution(layer_results,logit_diff_directions)\n",
    "\n",
    "plot_logit_attribution(component_logit_attribution,component_captions)\n",
    "plot_logit_attribution(layer_logit_attribution,layer_captions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Investigate 1_mlp_out further**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron_results = einops.rearrange(\n",
    "    batch_cache.get_neuron_results(layer=1),\n",
    "    \"batch n_vertices n_neurons d_model -> n_neurons batch n_vertices d_model\"\n",
    ")\n",
    "\n",
    "neuron_logit_attribution = get_logit_attribution(neuron_results,logit_diff_directions)\n",
    "\n",
    "plot_logit_attribution(neuron_logit_attribution,[str(i) for i in range(model.cfg.d_mlp)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Make datasets of specific examples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_kwargs = {\n",
    "    'n': model.cfg.n_vertices,\n",
    "    'size': 1099,\n",
    "    'start': 15,\n",
    "    'end': 22\n",
    "}\n",
    "\n",
    "planar_graphs = generate_planar(**dataset_kwargs)\n",
    "\n",
    "non_planar_graphs = generate_planar(**dataset_kwargs, non_planar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "misclassified_graphs, correct_labels = generate_misclass(\n",
    "    model=model,\n",
    "    start=15,\n",
    "    end=17,\n",
    "    size=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for adj in misclassified_graphs:\n",
    "    g = Graph(adj)\n",
    "    is_planar,cert = nx.check_planarity(g.G, counterexample=True)\n",
    "    print(is_planar)\n",
    "    Graph(cert).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Study average activations across sets of specific examples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_use_attn_result(True)\n",
    "_, planar_cache = model.run_with_cache(planar_graphs)\n",
    "_, non_planar_cache = model.run_with_cache(non_planar_graphs)\n",
    "_, misclassified_cache = model.run_with_cache(misclassified_graphs)\n",
    "model.set_use_attn_result(False)\n",
    "\n",
    "plot_acts(model,planar_cache)\n",
    "plot_acts(model,non_planar_cache)\n",
    "plot_acts(model,misclassified_cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logit attributions across sets of specific examples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "planar_neuron_results = einops.rearrange(\n",
    "    planar_cache.get_neuron_results(layer=1),\n",
    "    \"batch n_vertices n_neurons d_model -> n_neurons batch n_vertices d_model\"\n",
    ")\n",
    "\n",
    "non_planar_neuron_results = einops.rearrange(\n",
    "    non_planar_cache.get_neuron_results(layer=1),\n",
    "    \"batch n_vertices n_neurons d_model -> n_neurons batch n_vertices d_model\"\n",
    ")\n",
    "\n",
    "misclassified_neuron_results = einops.rearrange(\n",
    "    misclassified_cache.get_neuron_results(layer=1),\n",
    "    \"batch n_vertices n_neurons d_model -> n_neurons batch n_vertices d_model\"\n",
    ")\n",
    "\n",
    "planar_logit_diff_directions = get_logit_diff_directions(model,torch.ones(dataset_kwargs['size'],dtype=torch.int64))\n",
    "non_planar_logit_diff_directions = get_logit_diff_directions(model,torch.zeros(dataset_kwargs['size'],dtype=torch.int64))\n",
    "misclassified_logit_diff_directions = get_logit_diff_directions(model,torch.zeros(len(misclassified_graphs),dtype=torch.int64))\n",
    "\n",
    "planar_neuron_logit_attribution = get_logit_attribution(planar_neuron_results,planar_logit_diff_directions)\n",
    "non_planar_neuron_logit_attribution = get_logit_attribution(non_planar_neuron_results,non_planar_logit_diff_directions)\n",
    "misclassified_neuron_logit_attribution = get_logit_attribution(misclassified_neuron_results,misclassified_logit_diff_directions)\n",
    "\n",
    "plot_logit_attribution(planar_neuron_logit_attribution,[str(i) for i in range(model.cfg.d_mlp)])\n",
    "plot_logit_attribution(non_planar_neuron_logit_attribution,[str(i) for i in range(model.cfg.d_mlp)])\n",
    "plot_logit_attribution(misclassified_neuron_logit_attribution,[str(i) for i in range(model.cfg.d_mlp)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aisf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
