{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.0 **Setup** (dont read, just run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import ModelConfig, Graph, get_dataloader\n",
    "from utils.interp import *\n",
    "from models import get_model\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import networkx as nx\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import einops\n",
    "from jaxtyping import Float\n",
    "import numpy as np\n",
    "import rich\n",
    "from rich.table import Table\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 **Aim** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspired by [Neel Nanda's modular addition paper](https://arxiv.org/pdf/2301.05217), I wanted to train a toy model to do a mathematical algorithm and use mechanistic interpretability techniques to attempt to reverse engineer as much of the model as possible. I needed a task simple enough that I could train a toy model to achieve good performance but complex enough that there would be non-trivial computation to discover within the model. I settled on the task of *classifying graphs by their planarity*.\n",
    "\n",
    "The purpose of this notebook is to follow my thought process and key results as I attempt to achieve this aim. There are interactive visualisations along the way so that you yourself can make the same discoveries that I did (and maybe even make some more!?). For people less familiar with mech interp, this process can serve as a hands-on introduction to some key methods for analysing models. For people with a bit more experience, this is a fun way to get straight into analysing a novel toy model and practice your mech interp skills!\n",
    "\n",
    "The only real prerequesite is a basic understanding of how transformers work (attention heads, the residual stream etc.). If at any point you feel like you are missing some architectural intuition, then I recommend taking a look at the \"Transformer Overview\" section of [this paper](https://transformer-circuits.pub/2021/framework/index.html)(although be careful to note any differences in notation between the paper and the notation I define in section 0.3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 **The Task**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A *graph* is a collection of vertices with edges connecting pairs of those vertices. \n",
    "\n",
    "We say vertices $i$ and $j$ are *adjacent* if there is an edge connecting them.\n",
    "\n",
    "If a vertex is adjacent to $d$ other vertices then we say it has *degree $d$*.\n",
    "\n",
    "We can uniquely specify a graph with its *adjacency matrix* $X$ defined as follows:\n",
    "\n",
    "$X_{ij}=\\begin{cases}1 \\quad\\text{if } i \\text{ and } j \\text{ are adjacent} \\\\0 \\quad\\text{otherwise}\\end{cases}$\n",
    "\n",
    "The `Graph` class holds both the adjacency matrix and the [NetworkX](https://networkx.org/) representation of a graph in a single python object, allowing us to easily create, manipulate and visualise our own graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vertices = 10\n",
    "n_edges = 17\n",
    "\n",
    "random_graph = Graph(nx.gnm_random_graph(n_vertices,n_edges))\n",
    "\n",
    "print(random_graph.X)\n",
    "random_graph.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A graph is *planar* if you can draw it in the plane without any of its edges crossing. We call such a drawing a *planar embedding*.\n",
    "\n",
    "Is the graph we just plotted planar? In many cases it is not immediately obvious from the plot&mdash;just becuase the plot we see has crossing edges doesn't mean we can't rearrange the vertices so that no edges cross.\n",
    "\n",
    "Algorithms to determine the planarity of a graph do exist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_planar, certificate = nx.check_planarity(random_graph.G)\n",
    "print(\"This graph {} planar\".format(\"is\" if is_planar else \"isn't\"))\n",
    "if is_planar:\n",
    "    print(\"Here is a planar embedding of the graph:\")\n",
    "    random_graph.plot(pos = nx.combinatorial_embedding_to_pos(certificate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means we can create a dataset of graphs labelled with their planarity and train a model to classify graphs based on this label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.3 **The Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Disclaimer: A few of the design choices in this section are not particularly principled. The aim wasn’t to create the best model for the task, it was to train a model that achieved decent accuracy by doing some non trivial computation related to the task that I could hopefully reverse engineer.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The toy architecure that I used was a two layer decoder-only transformer with 8 attention heads per layer and a 128 dimensional residual stream.\n",
    "\n",
    "$n_{heads} = 8, d_{model} = 128$\n",
    "\n",
    "I used a dataset of ~100k adjacency matrices of graphs on 10 vertices.\n",
    "\n",
    "<details>\n",
    "    <summary> Dataset generation details, feel free to skip! </summary>\n",
    "The dataset was generated as follows:\n",
    "\n",
    "- Let $s$ be the desired size of the dataset and let $M = \\{15,16,...,20\\}$ be the set of edge counts. \n",
    "- For each $m$ in $M$, generate random graphs with $m$ edges until you have $s/2|M|$ planar graphs and $s/2|M|$ non-planar graphs.\n",
    "\n",
    "The reason to ensure a 50-50 split of planar and non-planar graphs across all $m$ values is because the probaility that a random graph (generated without forcing a 50-50 split) is planar is correlated to $m$. This means the model could use $m$ as a heuristic and classify based purely on that, not learning any computation fundementally related to planar graphs. Therefore I decided to prevent this possibility.\n",
    "\n",
    "The reason to use $M = \\{15,16,...,20\\}$ is because outside of this range the probability a random graph is planar is either very close to 0 or 1 so it is harder to generate $s/2|M|$ of both types. The model trained ends up having little problem generalising to outside this range.\n",
    "</details>\n",
    "\n",
    "Each adjacency matrix, $X$, (with shape $(10, 10)$) is embedded using matrices  $W_E$ (shape $(10, 128)$) and $W_{pos}$ (shape $(10, 128)$) as follows:\n",
    "\n",
    "$x_0 = XW_E + W_{pos},$ \n",
    "\n",
    "where $x_i$ is the residual stream after $i$ layers of the model. We refer to $XW_E$ as the contribution from the *Embed* and $W_{pos}$ as the contribution from the *PosEmbed*. Effectively, each vertex position in the residual stream pre layer 0, $x_0$, starts as the sum of the embeddings of its adjacent vertices (“looked up” in the $W_E$) as well as its own positional embedding from $W_{pos}$.\n",
    "\n",
    "The model outputs logits by mean pooling all positions in the residual stream, $x_2$, together into a 128 dimensional vector and then right multiplying by $W_U$ (shape $(128, 2)$)\n",
    "\n",
    "The trained model achieved 99.9% validation accuracy. I compared this with a few simpler architectures (e.g. mlp, see dropdown) to test whether my toy model was needlessly complex and concluded that the architecture did actually help with performance. Hopefully this means there is some non trivial computation to discover. \n",
    "\n",
    "<details>\n",
    "    <summary>Model training curves</summary>\n",
    "\n",
    "Comparing the training curves for my model with simpler alternatives suggests that my model has some sophisticated computation for us to discover. \n",
    "\n",
    "![](images/train_curve.png)\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are nearly ready to start interpreting the model! All that is left to do is load the model in and test it on a few examples to demonstrate that it is working properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'trained_models/transformer_2024-09-05_13-28-07'\n",
    "\n",
    "with open(path+'.yaml','r') as file:\n",
    "    args = yaml.safe_load(file)\n",
    "\n",
    "args['device'] = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = get_model(args)\n",
    "model.eval()\n",
    "state_dict = torch.load(path+'.pt',map_location=model.cfg.device, weights_only=True)\n",
    "model.load_state_dict(state_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test model on val set and random graphs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = get_dataloader('data/n10_15-21_tn_vn/val.npz',1)\n",
    "n_planar = n_correct = 0\n",
    "for x,y in loader:\n",
    "    logits = model(x)\n",
    "    correct = logits.argmax()==y\n",
    "    n_planar += y.item()*correct\n",
    "    n_correct += correct\n",
    "\n",
    "print(f\"Val Set Accuracy: {(100*n_correct/len(loader)).item():.2f}%\")\n",
    "print(f\"{(100*n_planar/n_correct).item():.2f}% of correctly classified graphs were planar.\")\n",
    "print()\n",
    "\n",
    "n_samples = 1000\n",
    "performace_table = Table(title='Model performance on random graphs:')\n",
    "performace_table.add_column('Number of edges',justify='center')\n",
    "performace_table.add_column(f'Percentage of {n_samples} random graphs correctly classified',justify='right')\n",
    "performace_table.add_column('Percentage of correctly classified graphs that were planar',justify='right')\n",
    "\n",
    "for m in range(10,25,3):\n",
    "    n_planar = n_correct = 0\n",
    "    for _ in range(n_samples):\n",
    "        g = Graph(nx.gnm_random_graph(model.cfg.n_vertices,m))\n",
    "        y = nx.is_planar(g.G)\n",
    "        logits = model(g.X)\n",
    "        correct = logits.argmax()==y\n",
    "        n_planar += y*correct\n",
    "        n_correct += correct\n",
    "    performace_table.add_row(str(m),f\"{(100*n_correct/n_samples).item():.2f}\",f\"{(100*n_planar/n_correct).item():.2f}\")\n",
    "\n",
    "rich.print(performace_table)\n",
    "                                          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. **Identifying a Circuit**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 **Cache activations from example graphs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to do is run a few example inputs through the model and cache the activations so that we can visualise what the model is doing to each input. The examples we use are: a few famous graphs, a batch of graphs from the validation set and the random graph we looked at earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "petersen_graph = Graph(nx.petersen_graph())\n",
    "cycle_graph = Graph(nx.cycle_graph(model.cfg.n_vertices))\n",
    "complete_graph = Graph(nx.complete_graph(model.cfg.n_vertices))\n",
    "empty_graph = Graph(nx.empty_graph(model.cfg.n_vertices))\n",
    "star_graph = Graph(nx.star_graph(model.cfg.n_vertices-1))\n",
    "\n",
    "loader = get_dataloader('data/n10_15-21_tn_vn/val.npz',1024)\n",
    "batch,batch_labels = next(iter(loader))\n",
    "\n",
    "model.set_use_attn_result(True)\n",
    "_, random_cache = model.run_with_cache(random_graph.X)\n",
    "_, petersen_cache = model.run_with_cache(petersen_graph.X)\n",
    "_, cycle_cache = model.run_with_cache(cycle_graph.X)\n",
    "_, complete_cache = model.run_with_cache(complete_graph.X)\n",
    "_, empty_cache = model.run_with_cache(empty_graph.X)\n",
    "_, star_cache = model.run_with_cache(star_graph.X)\n",
    "_, batch_cache = model.run_with_cache(batch)\n",
    "model.set_use_attn_result(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_planar = 0\n",
    "for g in batch:\n",
    "    graph = Graph(g)\n",
    "    n_planar += nx.is_planar(graph.G)\n",
    "print(f\"{100*n_planar/len(batch):.2f}% of the batch are planar graphs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 **Plot attention patterns**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by looking at the attention patterns for specific inputs to the model. In the case of our model, attention patterns $A^h$ are 10 by 10 matrices that represent how each attention head $h$ moves information between the vertex positions of the input graph. For instance, if $A_{ij}^{1.3}$ is large, that means that head 1.3 moves lots of information from vertex position $j$ to vertex position $i$. We say that vertex position $i$ *attends strongly* to vertex postion $j$.\n",
    "\n",
    "I took the usual [CircuitsVis](https://github.com/TransformerLensOrg/CircuitsVis) way of visualising language model attention patterns (in which you visualise which tokens in an input sentence attend to each other by shading the tokens themselves) and applied it to graph inputs. This results in a visualisation in which you can select an attention pattern and hover over a vertex to see which other vertices it attends to. \n",
    "\n",
    "It is important to remember that transformers are designed so that the attention patterns in each attention head *depend on the input*. Indeed, if we rename all the vertices of a graph but keep all the edges the same, we still want the vertices to attend to each other in the same way and so the model must adjust its attention patterns accordingly. It is therefore important to study the attention patterns for a whole range of input graphs. This way you can get a much better idea of what each head could be doing.\n",
    "\n",
    "Have a go yourself at looking at the attention patterns for different input graphs. The example below is for the attention patterns in layer 0 for a randomly generated graph, but you can change the layer and swap in any of the graphs that we cached during section 1.0.\n",
    "\n",
    "<details>\n",
    "    <summary>How to use this visualisation</summary>\n",
    "\n",
    "- Select the layer (0 or 1) of the model by changing the `layer` variable below.\n",
    "- Select the graph input by changing the `graph` and `cache` variables (e.g. try `graph=petersen_graph` and `cache=petersen_cache`).\n",
    "- Run the cell.\n",
    "- Select a head $h$ by clicking on one of the attention patterns $A^h$. This \"applies\" this attention pattern to the graph plot.\n",
    "- Hover over vertex $i$ in the graph to visualise the value $A^h_{ij}$ on every vertex $j$ in the graph. This allows us to see all the vertices in the graph from vertex $i$'s \"perspective\".\n",
    "</details> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 0\n",
    "graph = random_graph\n",
    "cache = random_cache\n",
    "\n",
    "graph.plot_attention(cache[\"pattern\",layer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: In Head 1.6, vertices of low degree attend stongly to vertex position 2. Let's try and work out how the model does this.\n",
    "\n",
    "My first guess as to how this attention pattern in head 1.6 comes about is:\n",
    "- Each vertex of low degree has the concept of \"I am a vertex of low degree\" encoded in its residual stream position. \n",
    "- The query matrix for head 1.6 reads this information from the residual stream and produces queries that encode the concept \"I am looking for the special vertex position\" in low degree vertex positions.\n",
    "- The key matrix for head 1.6 produces a key that says \"I am in the special vertex position\" in position 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 **Patching**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this theory by looking at how earlier components contribute to the attention score in head 1.6.\n",
    "\n",
    "We are going to take a \"corrupted\" graph with (in some sense) no low degree vertices as model input and then \"patch over\" the paths leading from a collection of earlier components to the query and/or the key of head 1.6 using the cached head 0.2 activations for a \"clean\" graph with some low degree vertices. This will allow us to see to what extent we can reproduce the \"clean\" behaviour of head 1.6 using only a subset of the components earlier in the model. We can use this to test my theory that degree information is passed into head 1.6 via the query and the positional information pinpointing vertex position 2 is passed in via the key. We should be able to go one step further and actually identify which earlier components are responsible for passing this information on to head 1.6. If we can find a subset of the model's components that are responsible for this then we will have identified the *circuit* responsible for head 1.6's function.\n",
    "\n",
    "By \"components\" I mean attention heads and the embedding. \n",
    "\n",
    "By \"path\" I mean the sequence of latent variables that lead from a component directly (without involving another component) to another component later in the model (in this case head 1.6).\n",
    "\"Patching over\" a path from component $A$ to component $B$ means effectively replacing all these latent varaibles with what they would be in the hypothetical scenario that component $A$'s output was replaced but all other components outputs stayed the same (even for components that come after $A$ in the model). By doing this we can get a better idea of the extent to which $A$ helps $B$ achieve its function. \n",
    "\n",
    "Notice that I choose not to include the mlps as components. Instead, we say that mlps count as part of the direct path between other components. This is because (in the case of this specific circuit) it turns out that all the important paths between other components go through the mlp on layer 0. Insisting on studying paths that skip the mlp obscures important relationships that depend on mlp computation.  \n",
    "\n",
    "Because you can't in general simultaneously patch two different paths during the same forward pass, we implement the above as follows:\n",
    "1. Choose two subsets of the components that come before head 1.6 in the model (i.e. Embed, Layer 1 heads)\n",
    "2. Input a \"corrupted\" graph into the model\n",
    "3. Replace only the output of the components within our first chosen subset with the equivalent output obtained when the \"clean\" graph is the input.\n",
    "4. Continue the model forward pass and cache the query of head 1.6.\n",
    "5. Begin another forward pass on the same corrupted output and this time replace only the output of the components within our *second* chosen subset.\n",
    "6. Continue the forward pass until layer 1 and replace the query in head 1.6 with the cached query from the previous run.\n",
    "7. Observe the resulting attention pattern in head 1.6 and hopefully pinpoint which components are needed to reproduce the \"clean\" attention pattern and in what way they compose with head 1.6.\n",
    "\n",
    "This is a specific implementation of a broader technique known as *path patching*. In the cells below we carry out path patching with the following configuration details:\n",
    "- `q_heads_to_patch` specifies heads $h$ for which the path from $h$ to the query of head 1.6 will be patched.\n",
    "- `q_patch_embed` specifies whether or not to patch the path from the Embed to the query of head 1.6.\n",
    "- `k_heads_to_patch` specifies heads $h$ for which the path from $h$ to the key of head 1.6 will be patched.\n",
    "- `k_patch_embed` specifies whether or not to patch the path from the Embed to the key of head 1.6.\n",
    "\n",
    "Have a play around with this configuration below and see if you can gain some insight that could help with step 7 above.\n",
    "\n",
    "<details>\n",
    "  <summary>More on patching</summary>\n",
    "  \n",
    "A more complete explanation of patching can be found by first reading about general [activation patching](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=qeWBvs-R-taFfcCq-S_hgMqx) and then [path patching](https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=M3vTmlndMJTgnL1-t1IAsTaR&q=path%20patching).\n",
    "  \n",
    "</details>\n",
    "\n",
    "<details>\n",
    "  <summary>Two things to note</summary>\n",
    "  \n",
    "PosEmbed is the same for all inputs so patching it in this scenario is pointless.\n",
    "\n",
    "The cell below runs path patching for several example configurations and, as such, the four variables described above are actually lists with the $i^{th}$ entry of each forming the configuration for the $i^{th}$ path patching example.\n",
    "  \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupted_input = petersen_graph.X\n",
    "corrupted_cache = petersen_cache\n",
    "clean_input = random_graph.X\n",
    "clean_cache = random_cache\n",
    "\n",
    "layer_1_head = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Components to patch:\n",
    "q_heads_to_patch = [[2],[3,4],[1,2,3]]\n",
    "q_patch_embed = [False,True,False]\n",
    "k_heads_to_patch = [[],[2],[1,2,3]]\n",
    "k_patch_embed = [False,False,False]\n",
    " \n",
    "component_lists = [q_heads_to_patch,q_patch_embed,k_heads_to_patch,k_patch_embed]\n",
    "\n",
    "n_examples = len(q_heads_to_patch)\n",
    "\n",
    "assert all(len(lst) == n_examples for lst in component_lists)\n",
    "\n",
    "px.imshow(random_cache[\"pattern\",1][0,6],color_continuous_scale='Blues',title=\"Clean Pattern\",width=1000).show()\n",
    "px.imshow(petersen_cache[\"pattern\",1][0,6],color_continuous_scale='Blues',title=\"Corrupted Pattern\",zmax=1,width=1000).show()\n",
    "\n",
    "for i in range(n_examples):\n",
    "    pattern = get_path_patched_attn_pattern(\n",
    "        model,\n",
    "        corrupted_input,\n",
    "        corrupted_cache,\n",
    "        clean_input,\n",
    "        clean_cache, \n",
    "        q_heads_to_patch = q_heads_to_patch[i],\n",
    "        q_patch_embed = q_patch_embed[i],\n",
    "        k_heads_to_patch = k_heads_to_patch[i],\n",
    "        k_patch_embed = k_patch_embed[i],\n",
    "        layer_1_head = layer_1_head\n",
    "    )\n",
    "\n",
    "    q_heads_string = \"heads 0.\" + \", 0.\".join(map(str, q_heads_to_patch[i])) if q_heads_to_patch[i] else \"\"\n",
    "    q_embed_string = \" Embed and\" if q_patch_embed[i] else \"\"\n",
    "    q_string = f\"{q_embed_string} {q_heads_string} -> query of head 1.6<br>\" if q_heads_string or q_embed_string else \"\"\n",
    "    k_heads_string = \"heads 0.\" + \", 0.\".join(map(str, k_heads_to_patch[i])) if k_heads_to_patch[i] else \"\"\n",
    "    k_embed_string = \" Embed and\" if k_patch_embed[i] else \"\"\n",
    "    k_string = f\"{k_embed_string} {k_heads_string} -> key of head 1.6<br>\" if k_heads_string or k_embed_string else \"\"\n",
    "    title = f\"Pattern when the following paths are patched:<br>{q_string}{k_string}\" \n",
    "\n",
    "    px.imshow(\n",
    "        pattern,\n",
    "        color_continuous_scale='Blues',\n",
    "        title=title,\n",
    "        zmax=1,\n",
    "        width=1000\n",
    "    ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having played around with the setup above, I made the following observations:\n",
    "1. I cannot reproduce any of the clean pattern purely by patching paths into the key of head 1.6. This suggests that some important information must be entering head 1.6 via the query. This does not rule out my theory that the information pinpointing vertex position 2 enters head 1.6 via the key (but it doesn't support it either).\n",
    "2. Configurations involving patching the path from head 0.2 to the query of head 1.6 tend to be the ones that reproduce the clean pattern most closely. This suggests that head 0.2 is passing important information to head 1.6 via the query. We say that head 0.2 is \"Q-composing\" with head 1.6.\n",
    "3. In configurations that involve head 0.2 but still miss some key parts of the pattern, adding in heads 0.1, 0.3 and/or Embed can help performance. This suggests more than one head is involved in the circuit responsible for the function of head 1.6. It also suggests that Embed has a direct effect on head 1.6, not just via layer 0 heads.\n",
    "\n",
    "To be more confident in observations 2 and 3, we can quantify how well the pattern has been reproduced and then repeat the patching process above for all possible configurations and across a whole batch of graphs, recording how well the pattern has been reproduced each time.\n",
    "When I say \"all possible configurations\" I am actually ignoring configurations involving patching paths from layer 0 heads to the key of head 1.6. This to reduce the number of possible configurations to a manageable amount. Based on observation 1, I am confident that this does not miss a configuration that offers a notably good reconstruction of the clean attention pattern.\n",
    "\n",
    "I quantify the pattern reconstuction performance using the `patching_metric` below (scaled so that 1 represents no improvement over the corrupted input and 0 represents perfect reconstruction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patching_metric(\n",
    "    reconstructed_pattern: Float[Tensor,\"batch n_vertices n_vertices\"],\n",
    "    clean_pattern: Float[Tensor,\"batch n_vertices n_vertices\"],\n",
    "    corrupted_pattern: Float[Tensor,\"batch n_vertices n_vertices\"] # batch can be 1 in some cases but broadcasting handles this\n",
    "):\n",
    "    return (torch.linalg.matrix_norm(reconstructed_pattern-clean_pattern)/torch.linalg.matrix_norm(clean_pattern-corrupted_pattern)).mean().item()\n",
    "\n",
    "def get_component_string(results_key):\n",
    "    \"\"\"Given key from results dict, return dict expressing which components that key corresponds to.\"\"\"\n",
    "    q_components = 'Embed '*results_key[0][0] + ' '.join(f'0.{head}' for head in results_key[0][1:])\n",
    "    k_components = 'Embed' if results_key[1][0] else ''\n",
    "    return q_components, k_components\n",
    "\n",
    "def get_n_components_patched(results_key):\n",
    "    \"\"\"Given key from results dict, find total number of components that have been patched\"\"\"\n",
    "    return sum([\n",
    "        results_key[0][0],\n",
    "        results_key[1][0],\n",
    "        len(results_key[0][1:]),\n",
    "        len(results_key[1][1:])\n",
    "    ])\n",
    "\n",
    "\n",
    "results = get_path_patching_metric_results(\n",
    "    model,\n",
    "    corrupted_input,\n",
    "    corrupted_cache,\n",
    "    batch,\n",
    "    batch_cache,\n",
    "    patching_metric,\n",
    "    layer_1_head\n",
    ")\n",
    "   \n",
    "table = Table('Number of components patched','Best Score Achieved','Q Components Patched','K Components Patched',title='Best paths to patch for each number of components patched:')\n",
    "one_component_table = Table(\"Q component\", \"K Component\",\"Pattern Reconstruction Score\", title='Results from patching one component')\n",
    "\n",
    "for n_components_patched in range(1,model.cfg.n_heads+2):\n",
    "    filtered_results = {k:v for k,v in results.items() if get_n_components_patched(k)==n_components_patched}\n",
    "    if n_components_patched==1:\n",
    "        for k,v in filtered_results.items():\n",
    "            q_component, k_component = get_component_string(k)\n",
    "            one_component_table.add_row(q_component, k_component,f\"{v:.2f}\")\n",
    "    best_combination = min(filtered_results, key=filtered_results.get)\n",
    "    best_q_components, best_k_components = get_component_string(best_combination)\n",
    "    best_metric_score = filtered_results[best_combination]\n",
    "    table.add_row(str(n_components_patched),f\"{best_metric_score:.2f}\",best_q_components,best_k_components)\n",
    "\n",
    "rich.print(one_component_table)\n",
    "rich.print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results above make me pretty confident that head 0.2 Q-composing with head 1.6 is the main interaction driving the circuit behind head 1.6's function.\n",
    "\n",
    "Note that we still haven't obtained any evidence supporting my theory that the information pinpointing vertex position 2 enters head 1.6 via the key. While it makes sense the key could have learned to produce a special key vector in position 2 that matches each vector that the query puts in a low degree vertex position, for all we know this information could be passed in via some other less intuitive mechanism. \n",
    "\n",
    "However, a quick check of the activations in the key of head 1.6 (averaged out across a batch of input graphs) strongly suggests that my theory is correct: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = batch_cache['k',1][:,:,6]\n",
    "px.imshow(k.mean(0),color_continuous_scale='RdBu',color_continuous_midpoint=0,title=\"Average key actiavtions for head 1.6\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key clearly knows which position head 1.6 wants to move information from. But how does it know this? From our observations, the choice of position 2 is independent of input and only depends on positional information. Therefore it would make sense if the information comes direct from the PosEmbed. \n",
    "\n",
    "We can test this with some more path patching. This time instead of patching a path using two different input graphs, we are going to patch the direct path from the positional embedding to the key of head 1.6 with the path that we would observe if the positional embeddings for position 2 and position 0 were swapped. We can then see the effect this has on the resulting attention pattern in head 1.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_to_swap_with_2 = 0\n",
    "\n",
    "new_pos_embed = model.W_pos.data.clone()\n",
    "new_pos_embed[[2,position_to_swap_with_2]] = new_pos_embed[[position_to_swap_with_2,2]]\n",
    "\n",
    "pattern = get_pos_embed_to_key_path_patched_attn_pattern(\n",
    "    model,\n",
    "    input=clean_input,\n",
    "    cache=clean_cache,\n",
    "    new_pos_embed=new_pos_embed,\n",
    "    layer_1_head=layer_1_head\n",
    ")\n",
    "\n",
    "title = f\"Pattern when the path from PosEmbed to the key of head 1.{layer_1_head}<br>is patched by switching position 2 and position {position_to_swap_with_2}.\"\n",
    "\n",
    "px.imshow(\n",
    "    pattern,\n",
    "    color_continuous_scale='Blues',\n",
    "    title=title,\n",
    "    zmax=1,\n",
    "    width=1000\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as we expect, we are able to consistently permute the columns of the head 1.6 pattern simply by permuting the rows of the PosEmbed and only allowing this change to effect head 1.6 via a direct path (not via layer 0 heads). This strongly suggests that the only information relevant to pinpointing position 2 in head 1.6 originates in the PosEmbed and enters the head via the key without any information being moved in between postions. Indeed, the mlp between the layer 0 and layer 1 heads acts positionwise so cannot move information and the attention heads are all effectively \"frozen\" by the path patching process.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results give us a pretty good intial overview of the circuit responsible for the function of head 1.6: the information required to pinpoint position 2 passes directly from the PosEmbed into head 1.6 via the key and the degree information passes from head 0.2 to head 1.6 via the query. A few other layer 0 heads also Q-compose with head 1.6 but are less important.\n",
    "<details>\n",
    "    <summary>A diagram to visualise this circuit</summary>\n",
    "\n",
    "The diagram below shows one way to visualise the circuit in the case that vertex 1 has low degree. Each copy of the model architecture represents the action of the model on a single vertex position in the residual stream. Any lines passing between copies represent information being moved between vertex positions. MLPs and irrelevant attention heads are omitted for simplicity. \n",
    "\n",
    "![](images/head16.png)\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 **Head 0.2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an initial overview, let's follow this trail back through the model by looking more closely at head 0.2. We will again start by studying the attention pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_graph.plot_attention(random_cache['pattern',0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: Vertices of high degree attend more to vertices 3 and 6, whereas vertices of low degree seem to attend evenly to all vertices.\n",
    "\n",
    "Although this pattern seems conceptually similar to head 1.6, there are three main differences that I observe:\n",
    "1. Instead of vertex position 2, vertices attend to vertex positions 3 and 6.\n",
    "2. Instead of low degree vertices, it is the high degree vertices that attend strongly to the special vertex positions.\n",
    "3. Instead of seemingly all \"low degree\" vertices attending maximally to the special vetex position (like in head 1.6), here it seems like attention scales with degree: the higher a vertex's degree, the more strongly it attends to vertex positions 3 and 6.\n",
    "\n",
    "Based on these observations, I have a similar theory for this heads mechansim as I did for head 1.6: the query receives the degree information and the key receives the positional information required for pinpointing positions 3 and 6.\n",
    "\n",
    "As before, we will now test this theory. However, now that we are studying a layer 0 head we have two simplifications that we can take advantage of:\n",
    "1. There are only two inputs to this head: the Embed and PosEmbed.\n",
    "2. There are no non-linear mlps between us and the model input. \n",
    "\n",
    "This means we can use some more straight forward techniques to study how the Embed and PosEmbed interact with head 0.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $X$ be an adjacnecy matrix model input, let $W_Q^h$ and $W_K^h$ be the query and key matrices for head $h$ and let $W_{pos}$ and $W_E$ be the positional embedding and embedding matrices. Then the query $q$ and key $k$ of head 0.2 are given by:\n",
    "\n",
    "$$q = XW_EW_Q^{0.2}+ W_{pos}W_Q^{0.2}$$\n",
    "\n",
    "$$k = XW_EW_K^{0.2}+ W_{pos}W_K^{0.2}$$\n",
    "\n",
    "This means we can nicely decompose $q$ and $k$ into their two terms and quantify how much each one contributes to the result. We will do this by taking the norm of each component of the sum and averaging out across a batch of inputs: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposed_qk_input_layer_0 = decompose_qk_input(model, batch_cache, layer=0)\n",
    "decomposed_q_layer_0 = decompose_q(model,decomposed_qk_input_layer_0,head_index=2,layer=0)\n",
    "decomposed_k_layer_0 = decompose_k(model,decomposed_qk_input_layer_0,head_index=2,layer=0)\n",
    "\n",
    "component_labels_layer_0 = [\"Embed\", \"PosEmbed\"]\n",
    "for decomposed_input, name in [(decomposed_q_layer_0, \"query\"), (decomposed_k_layer_0, \"key\")]:\n",
    "    px.imshow(\n",
    "        decomposed_input.pow(2).sum(-1).sqrt().mean(0).detach(),\n",
    "        labels={\"x\": \"Position\", \"y\": \"Component\"},\n",
    "        title=f\"Norms of components of {name}\",\n",
    "        y=component_labels_layer_0,\n",
    "        color_continuous_scale='Blues',\n",
    "        width=1000, height=400,\n",
    "        zmin=0\n",
    "    ).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, these plots suggest that all the degree information enters head 0.2 via the query. Indeed, degree information can only come from the Embed and when averaged out across a whole batch, we expect this information to be even distributed across vertex positions. This matches what we observe in the plots above: specifically a solid dark blue stripe across the Embed row in the first plot as well as a very light blue strip in the same place in the second plot, suggesting that the key cannot be using much degree information. \n",
    "\n",
    "These plots also support my theory that the positional information pinpointing vertex positions 3 and 6 enters via the key. Indeed the second plot suggests that the information in these postions in the PosEmbed is consistently of particular importance to the key but the query seems to give similar importance to all positions in PosEmbed.\n",
    "\n",
    "One additional observation of interest is that, as well as the degree information from the Embed, it seems that there is quite a lot of information from the PosEmbed entering head 0.2 via the query (suggested by the blue stripe across the PosEmbed row of the first plot). While this does not contradict any of my theories, it does suggest that there could be something more happening in head 0.2. Similarly, the are positions other than 3 and 6 in the key with reasonably sized PosEmbed component norms, also suggesting that there could be a more subtle purpose to head 0.2 as well.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One problem with the visualisations above is that, while it is believable that it is correlated with the importance of a component, the norm obscures a lot of the detail when it comes to how the PosEmbed and Embed effect head 0.2. We can get a better idea of their effect by recalling that the attention scores for head 0.2 are given by $qk^{T}/\\sqrt{d_{head}}$. This means we can use our decompositions of $q$ and $k$ from before to decompose the attention scores into four terms corrseponding to the contribution of each pairing of the two query components and the two key components to the overall attention score. This gives us a clearer idea of the impact each pairing of components has.\n",
    "\n",
    "For this visualisation we have to specifiy a single graph input with `batch_sample_idx` as opposed to avergaing over a batch. Indeed, averaging over the norms makes some sense but attention scores are very input dependent so averaging over them would just blur a lot of detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposed_scores_layer_0 = decompose_attn_scores(decomposed_q_layer_0, decomposed_k_layer_0)\n",
    "\n",
    "plot_decomposed_attn_scores(\n",
    "    decomposed_scores=decomposed_scores_layer_0,\n",
    "    cache=batch_cache,\n",
    "    component_labels=component_labels_layer_0,\n",
    "    layer=0,\n",
    "    head_idx=2,\n",
    "    batch_sample_idx=100,\n",
    "    zmax=3.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These plots offer much more support to my theory. The most important interactions for creating the attention pattern in head 0.2 are clearly the degree information passing from the Embed to the head via the query and the positional information passing from the PosEmbed to the head via the key. \n",
    "\n",
    "However, as we suspected, this is not the whole story. In particular, it seems that the query uses information from the PosEmbed to uniformly subtract from the attention scores in positions 3 and 6 (shown in the bottom right of the grid of four plots above). If the head's purpose is simply to allow vertices to attend to postions 3 and 6 based on their degree (in line with our observations), then this uniform subtraction seems unecessary. Is this just a quirk of the model or does it represent some key computation with an as yet unkown puropse? We will explore this more in the next section.   \n",
    "\n",
    "<details>\n",
    "    <summary>A diagram to visualise this part of the circuit</summary>\n",
    "    \n",
    "Just as we did earlier, we can visualise this part of the circuit, this time in the case that vertex 4 has high degree. Here we denote the contribution from the PosEmbed to the query with a lighter shade of green.\n",
    "\n",
    "![](images/head02.png)\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. **Zooming in**: following the circuit step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a broad overview of which components are composing to form the circuit responsible for head 1.6's function, it is time to attempt to follow this circuit through the network from input to head 1.6 in more detail.\n",
    "\n",
    "Hopefully by doing this we can answer questions like:\n",
    "- How exactly does $W_Q^{0.2}$ give the query vectors a concept of degree based on $XW_E$?\n",
    "- What does the mlp layer do to the information moving between heads 0.2 and 1.6?\n",
    "\n",
    "We begin by trying to answer the first of these questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this section we focus on one single input which we set below. I encourage you to swap this out for another graph that we cached during section 1.0 in order to see how the circuit behaves for different inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = random_graph\n",
    "cache = random_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 **Input -> Head 0.2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the query vectors are given by:\n",
    "\n",
    "$q = XW_EW_Q^{0.2}+ W_{pos}W_Q^{0.2}$,\n",
    "\n",
    "and that multiplying by $X$ is just summing the rows of $W_EW_Q^{0.2}$ corresponding to the vertices adjacent to each vertex. \n",
    "\n",
    "This means that studying the rows of $W_EW_Q^{0.2}$ and $W_{pos}W_Q^{0.2}$ will help us decompose what $W_Q^{0.2}$ does into smaller parts instead of looking at what it does to the entire residual stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 0\n",
    "head = 2\n",
    "\n",
    "W_E = model.W_E.detach()\n",
    "W_pos = model.W_pos.detach()\n",
    "W_Q = model.W_Q[layer,head].detach()\n",
    "W_K = model.W_K[layer,head].detach()\n",
    "resid = cache['resid_pre',0][0]\n",
    "\n",
    "px.imshow(W_E,color_continuous_scale='RdBu',title='W_E',width=1500).show()\n",
    "px.imshow(W_pos,color_continuous_scale='RdBu',title='W_pos',width=1500).show()\n",
    "px.imshow(W_Q,color_continuous_scale='RdBu',title=f'W_Q (head 0.{head})',width=1500).show()\n",
    "px.imshow(W_E@W_Q,color_continuous_scale='RdBu',title=f'W_E @ W_Q (head 0.{head})',width=1500,zmax=0.5,zmin=-0.5).show()\n",
    "px.imshow(W_pos@W_Q,color_continuous_scale='RdBu',title=f'W_pos @ W_Q (head 0.{head})',width=1500,zmax=0.5,zmin=-0.5).show()\n",
    "px.imshow(resid@W_Q,color_continuous_scale='RdBu',title=f'q (= resid @ W_Q) (head 0.{head})',width=1500).show()\n",
    "graph.plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these plots we can observe that, given a vertex in position $i$ with degree $d_i$, $W_Q^{0.2}$ maps the embedding of all $d_i$ adjacent vertices (found in the rows of $W_E$) to pretty much the same vector $\\mathbf{v}$ (represented by the verticle stripes in the plot of $W_EW_Q^{0.2}$ above). It also maps the positional embedding of $i$ (found in the rows of $W_{pos}$) to $-2\\mathbf{v}$. As described earlier, multiplying by $X$ sums the results in the rows of $W_EW_Q^{0.2}$ meaning that row $i$ of $XW_EW_Q^{0.2}$ contains the vector $d_i\\mathbf{v}$. By then summing the results with $W_{pos}W_Q^{0.2}$ we are subtracting $2\\mathbf{v}$ meaning the query of head 0.2 has the information $d_i-2$ stored in each vertex position $i$. \n",
    "\n",
    "By identifying this direction in the latent space of head 0.2's QK circuit corresponding to the concept of degree, we can read off the degree of each vertex directly from the query $q$ plotted above.  \n",
    "\n",
    "Given this insight, we now expect the key vectors in positions 3 and 6 of head 0.2 to also point in the direction $\\mathbf{v}$, therefore causing vertices with higher degree to attend more strongly to those positions. This means that the $W_{pos}W_K^{0.2}$ should make up the majority of the key $k$ (in line with the earlier decomposition analysis). Sure enough, when we plot $W_EW_K^{0.2}$ and $W_{pos}W_K^{0.2}$, our expectations are met: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.imshow(W_K,color_continuous_scale='RdBu',title=f'W_K (head 0.{head})',width=1500).show()\n",
    "px.imshow(W_E@W_K,color_continuous_scale='RdBu',title=f'W_E @ W_K (head 0.{head})',width=1500,color_continuous_midpoint=0,zmax=1.25,zmin=-1.25).show()\n",
    "px.imshow(W_pos@W_K,color_continuous_scale='RdBu',title=f'W_pos @ W_K (head 0.{head})',width=1500,color_continuous_midpoint=0,zmax=1.25).show()\n",
    "px.imshow(resid@W_K,color_continuous_scale='RdBu',title=f'k (= resid @ W_K) (head 0.{head})',width=1500).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 **Head 0.2 -> Head 1.6**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having reverse engineered the computation behind head 0.2's attention pattern, we now look at how that pattern is used to write information out to the residual stream. If $W_V^h$ is the value matrix for head $h$, then $W_V^{0.2}$ essentially appears to be doing that same thing as $W_K^{0.2}$, except with a different direction corresponding to degree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_V = model.W_V[layer,head].detach()\n",
    "z = cache['z',layer][0,:,head]\n",
    "result = cache['result',layer][0,:,head]\n",
    "\n",
    "px.imshow(W_V,color_continuous_scale='RdBu',title=f'W_V (head 0.{head})',color_continuous_midpoint=0).show()\n",
    "px.imshow(W_E@W_V,color_continuous_scale='RdBu',title=f'W_E @ W_V (head 0.{head})',color_continuous_midpoint=0).show()\n",
    "px.imshow(W_pos@W_V,color_continuous_scale='RdBu',title=f'W_pos @ W_V (head 0.{head})',color_continuous_midpoint=0).show()\n",
    "px.imshow(resid@W_V,color_continuous_scale='RdBu',title=f'v (= resid @ W_V) (head 0.{head})',color_continuous_midpoint=0).show()\n",
    "px.imshow(z,color_continuous_scale='RdBu',title=f'z (attention pattern applied to v) (head 0.{head})',color_continuous_midpoint=0).show()\n",
    "px.imshow(result,color_continuous_scale='RdBu',title=f'result (output of head 0.{head})',color_continuous_midpoint=0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these plots we can see how the value has the attention pattern applied to it (see z plot) and then is written to the residual stream (see result plot). In every plot we can see similar patterns. This because every row encodes degree in the same direction in the corresponding latent space. We now have a good understanding of how head 0.2 writes degree information to the residual stream for head 1.6 to read and use for its task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Between heads 0.2 and 1.6 we have an mlp layer, which makes it hard to track what happens to the output of a specific head. Indeed, because the mlp is non-linear we can't decompose the post mlp residual stream into nice components like we did for the residual stream input into the layer 0 attention heads.\n",
    "In the general case, just because a layer 0 head outputs a vector in the residual stream pointing in one direction does not mean this vector will be available to be read by a layer 1 head&mdash;the mlp can easily mess it up. \n",
    "\n",
    "This is a problem for head 1.6. As we have seen above, head 0.2 (among others) has written a result to the residual stream that contains the degree of each vertex as a scaled version of the same vector in each vertex position. In other words, we have an approximate direction in the residual stream that corresponds to the degree of a vertex. For head 1.6's purposes, it would be nice if the mlp did not tamper with this direction at all. Unfotunately, the mlp has lots of other jobs to do and so this nice interpretable direction appears to be lost. To visualise this, we plot two things below:\n",
    "1. The output obtained when you pass the output of head 0.2 ('result' plotted above) through the mlp. You can think of this as the \"theortecial output of the mlp if the output of head 0.2 was the only information the model wanted to pass to the next layer\".\n",
    "2. The full residual stream after the mlp on layer 0 below.\n",
    "\n",
    "Compare these plots with the 'result' plot above to see the effect the mlp's non-linearity has on one head's output vs when it has to handle a full residual streams worth of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_0 = model.blocks[0].mlp\n",
    "\n",
    "post_mlp = (mlp_0(result)+result).detach()\n",
    "\n",
    "\n",
    "px.imshow(post_mlp,color_continuous_scale='RdBu',title=\" Theoretical/fake 'component' of post mlp 0 residual stream orignating from head 0.2\",color_continuous_midpoint=0).show()\n",
    "px.imshow(cache['resid_post',0][0],color_continuous_scale='RdBu',title='Post mlp 0 residual stream',color_continuous_midpoint=0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"theoretical\" plot shows that the mlp seems to have a linear effect on the output of head 0.2 (the degree direction is rotated and scaled but the relative sizes of the vector pointing in the degree direction in each vertex position remain the same). We can still easily \"read off\" the degree information from the output of the mlp. Unfotunately, the actual output of the mlp (in the next plot) suggests that in general we probably wont have one residual direction from which to \"read off\" information like a vertex's degree. The actual residual stream appears to be very noisy.\n",
    "\n",
    "However, in this specific case, I can't ignore the similarity between the entries in the 68th basis direction ($e_{68}$) of the residual stream across all of the last three plots. It might just be a coincidence, but the pattern in that column matches the degree information we want to pass on (low values corresponding to high degree vertices and vice versa). Have a go at switching the input graph at the start of this section and seeing if you can replicate this observation in other cases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this could well be an interesting observation, it only has any relevance to head 1.6 if it actually takes adavntage of this apparent preservation of the degree direction by reading information from $\\langle e_{68}\\rangle$ (the subspace of the residual stream spanned by $e_{68}$). Let's plot $W_Q^{1.6}$ to get a better idea of where the query information is read in from (we will plot its transpose for easier visualisation, but remember it has shape (128, 32)): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_Q_1_6 = model.W_Q[1,6].detach()\n",
    "\n",
    "px.imshow(W_Q_1_6.T,color_continuous_scale='RdBu',title='W_Q (head 1.6)').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot makes me pretty confident that what we observed before is not a coincidence and that the non-linear mlp is preserving the degree pattern in a linear way by rotating and scaling it to roughly line up with $e_{68}$ in the residual stream.\n",
    "\n",
    "We can gather more evidence for this by taking a batch of input graphs and, for each graph, doing the following:\n",
    "1. Consider the output of head 0.2 and find the residual direction corresponding to degree that this head writes to.\n",
    "2. For each vertex find the coefficient of this direction vector that scales the vector, encoding the degree of the vertex.\n",
    "3. Form the 10 dimensional vector of these coefficients. This can be thought of as the \"degree signature\" of that specific graph and can be clearly visualised by looking at the columns of the output of head 0.2 ('result' plot above).\n",
    "4. Record the cosine similarity between the degree signature and each column of the post mlp residual stream.\n",
    "\n",
    "Once completed for each graph, average these results across the whole batch.\n",
    "\n",
    "These cosine similarities tell us the extent to which the relative sizes of the vectors in the \"degree direction\" pre mlp match the relative sizes of the vectors in $\\langle e_i\\rangle$ post mlp (for each $i$ in $[0,...,127]$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_degree_signature_resid_basis_cossim(pre_mlp,post_mlp):\n",
    "    return einops.einsum(\n",
    "        pre_mlp/pre_mlp.norm(dim=1).unsqueeze(1),post_mlp/post_mlp.norm(dim=1).unsqueeze(1),\n",
    "        \"batch n_vertices d_model, batch n_vertices d_model -> d_model\"\n",
    "    ).detach()/pre_mlp.shape[0]\n",
    "\n",
    "post_mlp = batch_cache['resid_pre',1]\n",
    "head_0_2_output_pre_mlp = batch_cache['result',0][:,:,2]\n",
    "\n",
    "cossims = get_degree_signature_resid_basis_cossim(head_0_2_output_pre_mlp,post_mlp)\n",
    "\n",
    "px.line(\n",
    "    y=cossims,\n",
    "    labels={\"x\": \"Resid stream basis direction\", \"y\": \"Degree signature cosine similarity\"},\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot shows us that the information held in $\\langle e_{68}\\rangle$ is, on average, very similar to the degree signature of the input graph. From this I conclude that the degree information is passed from head 0.2 to the query of head 1.6 via the mlp which rotates the \"degree direction\" in the residual stream to allign with $e_{68}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now follow this degree information into head 1.6. We can observe how $W_Q^{1.6}$ reads the degree information from the residual stream to create a query $q$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.imshow(cache['resid_pre',1][0]@W_Q_1_6,color_continuous_scale='RdBu',title='q (= resid @ W_Q) (head 1.6)',width=1500).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to head 0.2, head 1.6 clearly has a direction corresponding to degree (shown by the stripes across the rows corresponding to low degree vertices). But unlike head 0.2 it seems like the direction is a little more noisy. Is there noise simply becuase this head is later in the model and comes after the non-linear mlp, or could it be because there is actually some extra information encoded in the query of this head?\n",
    "\n",
    "Whether $W_Q^{1.6}$ does some extra computation or not, we can now clearly see the vertices that head 1.6 wants to move information to, right there in the query. All that's left is to observe how the key creates a vector in position 2 that matches this \"low degree vertex\" direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike $W_Q^{1.6}$, $W_K^{1.6}$ does not appear to be reading from a small set basis directions in the residual stream:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_K_1_6 = model.W_K[1,6].detach()\n",
    "\n",
    "px.imshow(W_K_1_6.T,color_continuous_scale='RdBu',title='W_K (head 1.6)').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes is more difficult to theorise exactly what the mlp is doing to the information that is then read into head 1.6 by $W_K^{1.6}$. However, as we saw in section 1, we know that this information predominantly comes directly from the PosEmbed and is used to create a key vector in position 2 that matches the query vectors in low degree vertex positions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.imshow(cache['resid_pre',1][0]@W_K_1_6,color_continuous_scale='RdBu',title='q (= resid @ W_K)',width=1500).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that, I think we have done enough to say that we have reverse engineered a large part of the circuit behind the function of head 1.6, we even followed the input step by step through each layer and identified the directions in each latent space that corresponded to degree. There are, of course, contributions to the input to head 1.6 that come from heads other than 0.2 that we have not considered in as much detail as head 0.2. I will discuss these in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. **Next Steps**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The obvious next question is: \"what does any of this have to do with the planarity of a graph?\". I am not sure at all. My best guess so far is that the model could be leveraging the fact that *vertices of degree less than 3 are not relevant to planarity*. Indeed, for vertices of degree 0 or 1, removing them from a graph is never going to change a graph's planarity. For vertices of degree 2, replacing them with a single edge between their adjacent vertices is never going to change a graph’s planarity. Perhaps the purpose of head 1.6 is to help the model ignore this redundant information when making its decision? If true, it could help explain the significance of $W_{pos}W_Q^{0.2}$ representing “-2 in the degree direction”: it could be creating a “cutoff” below which all vertices should be in some sense ignored. Indeed, in most cases this \"cutoff\" appears to be maintained from head 0.2 to 1.6 and becomes the point at which query vectors in head 1.6 switch from pointing in the same direction as the key vector in positon 2 (causing strong attention scores) to pointing in the opposite direction (causing low attention scores).\n",
    "\n",
    "To add further speculation to this theory, note the following:\n",
    "\n",
    "While most of the time head 1.6 seems to pick out only vertices of degree 2 or less, there are some graphs for which vertices with higher degree also receive information from position 2.\n",
    "\n",
    "Here are two examples (look at vertex 9 in the first and vertex 1 in the second):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.tensor([[\n",
    "    [0., 1., 1., 0., 1., 1., 0., 0., 0., 1.],\n",
    "    [1., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
    "    [1., 0., 0., 0., 1., 1., 1., 0., 1., 0.],\n",
    "    [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
    "    [1., 0., 1., 0., 0., 0., 0., 0., 0., 1.],\n",
    "    [1., 1., 1., 0., 0., 0., 0., 1., 1., 0.],\n",
    "    [0., 0., 1., 0., 0., 0., 0., 0., 1., 0.],\n",
    "    [0., 0., 0., 0., 0., 1., 0., 0., 1., 0.],\n",
    "    [0., 0., 1., 0., 0., 1., 1., 1., 0., 0.],\n",
    "    [1., 0., 0., 1., 1., 0., 0., 0., 0., 0.]\n",
    "]])\n",
    "\n",
    "B = torch.tensor(([\n",
    "    [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "    [1., 0., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
    "    [0., 1., 0., 0., 1., 1., 1., 1., 1., 1.],\n",
    "    [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "    [0., 1., 1., 0., 0., 1., 1., 0., 1., 0.],\n",
    "    [0., 0., 1., 0., 1., 0., 1., 1., 0., 1.],\n",
    "    [0., 0., 1., 0., 1., 1., 0., 1., 1., 1.],\n",
    "    [0., 0., 1., 0., 0., 1., 1., 0., 1., 0.],\n",
    "    [0., 0., 1., 0., 1., 0., 1., 1., 0., 1.],\n",
    "    [0., 0., 1., 0., 0., 1., 1., 0., 1., 0.]\n",
    "]))\n",
    "\n",
    "\n",
    "graph_to_use = A\n",
    "\n",
    "exception_graph = Graph(graph_to_use)\n",
    "\n",
    "model.set_use_attn_result(True)\n",
    "_, exception_cache = model.run_with_cache(exception_graph.X)\n",
    "model.set_use_attn_result(False)\n",
    "\n",
    "exception_graph.plot_attention(exception_cache['pattern',1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does vertex 9 in graph A attend strongly to position 2 when it has degree 3? What about vertex 1 in graph B, it has degree 4!? A clue is to look at the attention pattern when we patch the path from head 0.2 to the query of head 1.6 (like we did in section 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Components to patch:\n",
    "q_heads_to_patch = [2]\n",
    "\n",
    "px.imshow(exception_cache[\"pattern\",1][0,6],color_continuous_scale='Blues',title=\"Clean Pattern\",width=1000).show()\n",
    "px.imshow(petersen_cache[\"pattern\",1][0,6],color_continuous_scale='Blues',title=\"Corrupted Pattern\",zmax=1,width=1000).show()\n",
    "\n",
    "pattern = get_path_patched_attn_pattern(\n",
    "    model,\n",
    "    petersen_graph.X,\n",
    "    petersen_cache,\n",
    "    exception_graph.X,\n",
    "    exception_cache, \n",
    "    q_heads_to_patch, \n",
    "    q_patch_embed=False, \n",
    "    k_heads_to_patch=[], \n",
    "    k_patch_embed=False, \n",
    "    layer_1_head=6\n",
    ")\n",
    "\n",
    "title = f\"Pattern when the path from heads {q_heads_to_patch} to the query of head 1.6 are patched.\"\n",
    "\n",
    "px.imshow(\n",
    "    pattern,\n",
    "    color_continuous_scale='Blues',\n",
    "    title=title,\n",
    "    zmax=1,\n",
    "    width=1000\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Head 0.2 manages help head 1.6 to detect all the vertices of degree 2 or below. This is in line with what we expect, given that we observed how head 0.2 effectively calculates the degree of each vertex and subtracts 2 (which subsequently contributes to the \"cutoff\" effect in head 1.6 described earlier). However, with head 0.2 alone, head 1.6 cannot detect vertex 9 from graph A nor vertex 1 from graph B. Experimenting with patching in different heads above shows us that the computation required in the layer 0 heads in order for head 1.6 to detect vertex 9 is much more nuanced and requires several heads working together. To reverse engineer this would require much more analysis.\n",
    "\n",
    "The next question is \"*why* do some vertices with degree higher than 2 attend strongly to position 2?\" This is seemingly in contradiction to our speculation that head 1.6 is helping the model \"ignore vertices of degree less than 3 because they dont effect planarity\". If these \"special\" vertices don't have degree less than 3 then why would the model want to ignore it? My theory is that the model has calculated that vertex 9 has degree less than 4 and has a neighbour that has degree less than 2. This means that vertex 9 also has no effect on the planarity of the graph, and so can be ignored. More generally, I speculate that head 1.6 is actually finding vertices $i$ satisfying the following condition:\n",
    "- Vertex $i$ is adjacent to $\\geqslant d_i-2$ vertices that each have degree less than 3 (where $d_i$ is the degree of vertex $i$).\n",
    "\n",
    "This condition is automatically met for all vertices $i$ with $d_i\\leqslant2$, regardless of which vertices they are adjacent to. From our observations we concluded that this case is handled mostly by head 0.2 and is the case that is most common and easy to observe. The other cases require further computation from other heads which is why we observe these cases missing from the pattern in head 1.6 when we only patch head 0.2.\n",
    "\n",
    "Whether this theory is true or not, it is clear from this example that there is more to discover about head 1.6.\n",
    "\n",
    "The next obvious steps are to:\n",
    "- Carry out analysis on other layer 0 heads similar to the analysis we did on head 0.2.\n",
    "- Try and better understand how the output of head 1.6 is used later in the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. **Conclusion** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While I haven't fully reverse engineered the circuit behind head 1.6, I believe I have made a good start and identified a promising direction to not only get a more complete picture of this circuit, but the function of the model overall. I hope that this notebook stands as a good example of how a complete mech interp beginner can get stuck in and find real, explainable, non-trivial computation within a model. I also hope that section 3 in particular demonstrates the need to question your intial theories and seek annoying edge cases that break them. This is a great way to guide your next steps and get closer to what it is actually going on!   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aisf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
